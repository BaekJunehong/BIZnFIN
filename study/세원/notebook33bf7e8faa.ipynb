{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1938459,"sourceType":"datasetVersion","datasetId":1111894}],"dockerImageVersionId":30055,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 회사 파업 📉💸\n대만 증권거래소의 비즈니스 규정에 따라 회사 파산을 나타내는 1999~2009년 대만 경제 저널의 데이터다.\n\n소개\n\n이 커널에서는 다양한 예측 모델을 사용하여 향후 파산에 직면할 기업을 올바르게 예측할 수 있는지 여부를 감지하는 데 얼마나 정확한지 확인할 것입니다. 데이터 섹션에 설명된 대로 데이터 세트에는 다음이 포함되어 있습니다:\n\n95개 기능(X1-X95, 대만 증권거래소 업무 규정)\n1 라벨 벡터\n이 프로젝트의 목표는 이러한 기능을 사용하여 선택된 모델에 미치는 영향/역할을 파악하고 뱅크럽에 가까운 기업을 인식하는 데 어떻게 도움이 될 수 있는지 파악하는 것입니다. 시작합니다!\n\n추신: 이 노트북이 마음에 드신다면 UPVOTE를 잊지 마세요!","metadata":{}},{"cell_type":"code","source":"# IMPORTING LIBRARIES\n\n# General Libraries\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\n# Preprocessing Libraries\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Machine Learning Libraries\n\nimport sklearn\nimport xgboost as xgb\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_curve\nfrom imblearn.pipeline import Pipeline\nfrom catboost import CatBoostClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.metrics import classification_report\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold, cross_validate\nfrom sklearn.metrics import recall_score, f1_score, roc_auc_score\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n\n\n\n# Defining the working directory\n\ninput_path = '../input/company-bankruptcy-prediction/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IMPORTING DATA\n\nbank_data = pd.read_csv(input_path + 'data.csv')\nbank_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이제 데이터에 대한 아이디어가 생겼으니 데이터에 대한 더 많은 정보를 얻어야 합니다. 가장 먼저 이해하고 싶은 것은 데이터의 특성, 즉 데이터가 수치 또는 범주형인지, 그리고 그 중에 누락된 정보가 있는지 여부입니다. info() 를 사용하여 이 두 가지 점을 모두 확인할 수 있습니다.\n\n결과 패널은 우리에게 강력한 정보를 제공하며, 그 방법을 보여줍니다:\n\n데이터 세트는 96개의 features 각각에 대해 6819개의 관찰 결과 조합으로 구성됩니다. 모든 features은 숫자(int64 또는 float64)입니다 데이터 중 누락된 값(Nan)이 없습니다 모든 features가 숫자이기 때문에 추가 정보 출처인 기술 통계를 쉽게 계산할 수 있습니다.","metadata":{}},{"cell_type":"markdown","source":"결과 패널은 우리에게 강력한 정보를 제공하며, 그 방법을 보여줍니다:\n\n데이터 세트는 96개의 기능 각각에 대해 6819개의 관찰 결과 조합으로 구성됩니다.\n모든 기능은 숫자(int64 또는 float64)입니다\n데이터 중 누락된 값(Nan)이 없습니다\n모든 기능이 숫자이기 때문에 추가 정보 출처인 기술 통계를 쉽게 계산할 수 있습니다.","metadata":{}},{"cell_type":"code","source":"# Computing the descriptive statistics of our numrerical features\n\nbank_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"누락된 값이 없다는 것을 이미 알고 있지만(여기서는 96개의 feature만 있다는 점을 고려하면 매우 쉽습니다), 프로젝트의 다음 단계에서 오류와 시간 낭비를 방지하려면 이것이 사실인지 계산적으로 확인하는 것이 중요합니다.","metadata":{}},{"cell_type":"code","source":"# Checking Nan presence\n\n#bank_data.isna().sum().max()\n[print(col) for col in bank_data if bank_data[col].isna().sum() > 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"데이터에 중복이 있을 가능성에 대해 추가로 고려해야 합니다. 중복은 데이터에 중복을 일으킬 수 있는 동일한 관찰이므로 삭제해야 합니다. 제거할 중복이 있는지 확인해 보겠습니다:","metadata":{}},{"cell_type":"code","source":"# Checking for duplicates\n\nbank_data.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"데이터에 중복이 있을 가능성에 대해 추가로 고려해야 합니다. 중복은 데이터에 중복을 일으킬 수 있는 동일한 관찰이므로 삭제해야 합니다. 제거할 중복이 있는지 확인해 보겠습니다:","metadata":{}},{"cell_type":"code","source":"# The classes are heavily skewed we need to solve this issue later.\n\nprint(bank_data['Bankrupt?'].value_counts())\nprint('-'* 30)\nprint('Financially stable: ', round(bank_data['Bankrupt?'].value_counts()[0]/len(bank_data) * 100,2), '% of the dataset')\nprint('Financially unstable: ', round(bank_data['Bankrupt?'].value_counts()[1]/len(bank_data) * 100,2), '% of the dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking labels distributions\n\nsns.set_theme(context = 'paper')\n\nplt.figure(figsize = (10,5))\nsns.countplot(bank_data['Bankrupt?'])\nplt.title('Class Distributions \\n (0: Fin. Stable || 1: Fin. Unstable)', fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"위의 플롯을 보면 레이블의 불균형이 얼마나 심한지 명확하게 알 수 있으며, 이는 좋은 성능을 얻기 위해 해결해야 할 주요 장애물입니다.","metadata":{}},{"cell_type":"markdown","source":"Exploratory Data Analysis (EDA).\n\n여러 유형의 시각화에서 가능한 한 많은 정보를 얻기 위해 데이터 세트를 살펴봅시다. 가장 먼저 보여줄 일반적인 플롯은 데이터 세트의 수치 변수 간의 모든 상관관계(선택한 테스트에 따라 선형 및 비 선형)를 플롯하는 상관관계 히트맵입니다: 피어슨 또는 스피어맨. 기능 간의 상관관계를 파악하는 것은 훈련 중에 유지해야 할 기능을 결정하는 데 중요한 단계입니다.","metadata":{}},{"cell_type":"code","source":"# Looking at the histograms of numerical data\n\nbank_data.hist(figsize = (35,30), bins = 50)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EDA & VISUALIZATIONS\n\n# Correlation Heatmap (Spearman)\n\nf, ax = plt.subplots(figsize=(30, 25))\nmat = bank_data.corr('spearman')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0,# annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Boxplots of the numerical features\n\nplt.figure(figsize = (20,20))\nax =sns.boxplot(data = bank_data, orient=\"h\")\nax.set_title('Bank Data Boxplots', fontsize = 18)\nax.set(xscale=\"log\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting interesting features\n\nf, axes = plt.subplots(ncols=4, figsize=(24,6))\n\nsns.boxplot(x='Bankrupt?', y=\" Net Income to Total Assets\", data=bank_data, ax=axes[0])\naxes[0].set_title('Bankrupt vs Net Income to Total Assets')\n\nsns.boxplot(x='Bankrupt?', y=\" Total debt/Total net worth\", data=bank_data, ax=axes[1]) \naxes[1].set_title('Bankrupt vs Tot Debt/Net worth Correlation')\n\n\nsns.boxplot(x='Bankrupt?', y=\" Debt ratio %\", data=bank_data, ax=axes[2])\naxes[2].set_title('Bankrupt vs Debt ratio Correlation')\n\n\nsns.boxplot(x='Bankrupt?', y=\" Net worth/Assets\", data=bank_data, ax=axes[3])  \naxes[3].set_title('Bankrupt vs Net Worth/Assets Correlation') \n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting other interesting features\n\nf, axes = plt.subplots(ncols=4, figsize=(24,6))\n\nsns.boxplot(x='Bankrupt?', y=\" Working Capital to Total Assets\", data=bank_data, ax=axes[0])\naxes[0].set_title('Bankrupt vs  working capital to total assets')\n\nsns.boxplot(x='Bankrupt?', y=\" Cash/Total Assets\", data=bank_data, ax=axes[1])\naxes[1].set_title('Bankrupt vs cash / total assets')\n\n\nsns.boxplot(x='Bankrupt?', y=\" Current Liability to Assets\", data=bank_data, ax=axes[2])\naxes[2].set_title('Bankrupt vs current liability to assets')\n\n\nsns.boxplot(x='Bankrupt?', y=\" Retained Earnings to Total Assets\", data=bank_data, ax=axes[3])\naxes[3].set_title('Bankrupt vs  Retained Earnings to Total Assets')\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이제 파산이 임박한 기업에 대한 이러한 기능의 분포를 살펴봅시다:","metadata":{}},{"cell_type":"code","source":"# Plotting the feature distributions for close to bankrputcy companies\n\nf, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n\ncash_flow_rate = bank_data[' Net Income to Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(cash_flow_rate,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title(' Net Income to Total Assets \\n (Unstable companies)', fontsize=14)\n\ntot_debt_net = bank_data[' Total debt/Total net worth'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(tot_debt_net ,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('total debt/tot net worth \\n (Unstable companies)', fontsize=14)\n\n\ndebt_ratio = bank_data[' Debt ratio %'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(debt_ratio,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('debt_ratio \\n (Unstable companies)', fontsize=14)\n\nnet_worth_assets = bank_data[' Net worth/Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(net_worth_assets,ax=ax4, fit=norm, color='#C5B3F9')\nax4.set_title('net worth/assets \\n (Unstable companies)', fontsize=14)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n\nworking_cap = bank_data[' Working Capital to Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(working_cap,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('working capitals to total assets \\n (Unstable companies)', fontsize=14)\n\ncash_tot_assets = bank_data[' Cash/Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(cash_tot_assets ,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('cash/total assets \\n (Unstable companies)', fontsize=14)\n\n\nasset_liab = bank_data[' Current Liability to Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(asset_liab,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('liability to assets \\n (Unstable companies)', fontsize=14)\n\noperating_funds = bank_data[' Retained Earnings to Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(operating_funds,ax=ax4, fit=norm, color='#C5B3F9')\nax4.set_title('retain earnings to total assets \\n (Unstable companies)', fontsize=14)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이상값 제거\n\n이 프로젝트에서는 가장 극단적인 이상값을 제거하려고 합니다(이를 제거하는 대신 평균 또는 중앙값으로 추정할 수도 있습니다). 이렇게 하면 모델의 성능이 향상될 것입니다.","metadata":{}},{"cell_type":"code","source":"# Outliers removal\n\ndef outliers_removal(feature,feature_name,dataset):\n    \n    # Identify 25th & 75th quartiles\n\n    q25, q75 = np.percentile(feature, 25), np.percentile(feature, 75)\n    print('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n    feat_iqr = q75 - q25\n    print('iqr: {}'.format(feat_iqr))\n    \n    feat_cut_off = feat_iqr * 1.5\n    feat_lower, feat_upper = q25 - feat_cut_off, q75 + feat_cut_off\n    print('Cut Off: {}'.format(feat_cut_off))\n    print(feature_name +' Lower: {}'.format(feat_lower))\n    print(feature_name +' Upper: {}'.format(feat_upper))\n    \n    outliers = [x for x in feature if x < feat_lower or x > feat_upper]\n    print(feature_name + ' outliers for close to bankruptcy cases: {}'.format(len(outliers)))\n    #print(feature_name + ' outliers:{}'.format(outliers))\n\n    dataset = dataset.drop(dataset[(dataset[feature_name] > feat_upper) | (dataset[feature_name] < feat_lower)].index)\n    print('-' * 65)\n    \n    return dataset\n\nfor col in bank_data:\n    new_df = outliers_removal(bank_data[col],str(col),bank_data)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"이제 청소된 상자 그림을 살펴봅시다:","metadata":{}},{"cell_type":"code","source":"f,(ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24,6))\n\n# Boxplots with outliers removed\n\nsns.boxplot(x=\"Bankrupt?\", y=\" Net Income to Total Assets\", data=new_df,ax=ax1) \nax1.set_title(\"Net Income to Total Assets \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=\" Total debt/Total net worth\", data=new_df,ax=ax2) \nax2.set_title(\"total debt/total net worth \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=\" Debt ratio %\", data=new_df,ax=ax3) \nax3.set_title(\"debt ratio % \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=' Net worth/Assets', data=new_df,ax=ax4) \nax4.set_title(\"net worth/assets \\n Reduction of outliers\", fontsize=14)\n        \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f,(ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24,6))\n\n# Boxplots with outliers removed\n\nsns.boxplot(x=\"Bankrupt?\", y=' Working Capital to Total Assets', data=new_df,ax=ax1) \nax1.set_title(\"working capital to total assets \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=' Cash/Total Assets', data=new_df,ax=ax2) \nax2.set_title(\"cash / total assets \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=' Current Liability to Assets', data=new_df,ax=ax3) \nax3.set_title(\"current liability to assets \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=' Retained Earnings to Total Assets', data=new_df,ax=ax4) \nax4.set_title(\"Retained Earnings to Total Assets \\n Reduction of outliers\", fontsize=14)\n        \nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the feature distributions for close to bankrputcy companies\n\nf, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n\ncash_flow_rate = new_df[' Net Income to Total Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(cash_flow_rate,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title(' Net Income to Total Assets \\n (Unstable companies)', fontsize=14)\n\ntot_debt_net = new_df[' Total debt/Total net worth'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(tot_debt_net ,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('total debt/tot net worth \\n (Unstable companies)', fontsize=14)\n\n\ndebt_ratio = new_df[' Debt ratio %'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(debt_ratio,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('debt_ratio \\n (Unstable companies)', fontsize=14)\n\nnet_worth_assets = new_df[' Net worth/Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(net_worth_assets,ax=ax4, fit=norm, color='#C5B3F9')\nax4.set_title('net worth/assets \\n (Unstable companies)', fontsize=14)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n\nworking_cap = new_df[' Working Capital to Total Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(working_cap,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('working capitals to total assets \\n (Unstable companies)', fontsize=14)\n\ncash_tot_assets = new_df[' Cash/Total Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(cash_tot_assets ,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('cash/total assets \\n (Unstable companies)', fontsize=14)\n\n\nasset_liab = new_df[' Current Liability to Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(asset_liab,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('liability to assets \\n (Unstable companies)', fontsize=14)\n\noperating_funds = new_df[' Retained Earnings to Total Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(operating_funds,ax=ax4, fit=norm, color='#C5B3F9')\nax4.set_title('retain earnings to total assets \\n (Unstable companies)', fontsize=14)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"우리가 볼 수 있는 것은 극단적인 이상값을 제거하면 더 많은 \"종 모양\" 분포를 얻는 데 확실히 도움이 된다는 것입니다! (적어도 표시된 features의 경우)","metadata":{}},{"cell_type":"code","source":"# Dividing Data and Labels\n\nlabels = new_df['Bankrupt?']\nnew_df = new_df.drop(['Bankrupt?'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def log_trans(data):\n    \n    for col in data:\n        skew = data[col].skew()\n        if skew > 0.5 or skew < -0.5:\n            data[col] = np.log1p(data[col])\n        else:\n            continue\n            \n    return data\n\ndata_norm = log_trans(new_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Boxplots of the preprocessed numerical features\n\nplt.figure(figsize = (20,20))\nax =sns.boxplot(data = data_norm, orient=\"h\")\nax.set_title('Bank Data Preprocessed Boxplots', fontsize = 18)\nax.set(xscale=\"log\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_norm.hist(figsize = (35,30),bins = 50)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting Train and Test Data\n\nX_raw,X_test,y_raw,y_test  = train_test_split(data_norm,\n                                              labels,\n                                              test_size=0.1,\n                                              stratify = labels,\n                                              random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SMOTE \n오버샘플링 기법으로 SMOTE를 사용하는 것이 성능을 향상시키는 데 도움이 될 수 있는지 알아보겠습니다. 무작위 언더샘플링보다 SMOTE가 더 정확할 가능성이 높지만, 앞서 언급한 것처럼 행이 제거되지 않기 때문에 훈련하는 데 더 많은 시간이 걸릴 것입니다.\n\nSMOTE알고리즘은 오버샘플링 기법 중 합성데이터를 생성하는 방식으로 가장 많이 사용되고 있는 모델이다 SMOTE(synthetic minority oversampling technique)란, 합성 소수 샘플링 기술로 다수 클래스를 샘플링하고 기존 소수 샘플을 보간하여 새로운 소수 인스턴스를 합성해낸다. 일반적인 경우 성공적으로 작동하지만, 소수데이터들 사이를 보간하여 작동하기 때문에 모델링셋의 소수데이터들 사이의 특성만을 반영하고 새로운 사례의 데이터 예측엔 취약할 수 있다.\n\nSMOTE 동작 방식 부트스트래핑이나 KNN(최근접이웃) 모델 기법을 활용한다.\n\n소수 데이터 중 특정 벡터 (샘플)와 가장 가까운 이웃 사이의 차이를 계산한다. 이 차이에 0과 1사이의 난수를 곱한다. 타겟 벡터에 추가한다. 두 개의 특정 기능 사이의 선분을 따라 임의의 점을 선택할 수 있다.","metadata":{}},{"cell_type":"markdown","source":"## MODELING\n\n\n이제 모델로 무엇을 할 수 있는지 살펴보겠습니다! 정리된 언더샘플링 데이터와 SMOTE 오버샘플링 데이터의 성능을 확인할 수 있습니다. 이 부분을 위해 저는 몇 가지 다른 모델을 사용하기로 결정했습니다:\n\nLogistic Regression SVC Random Forest Classifier CatBoost Classifier\n\n* Logistic Regression\n* SVC \n* Random Forest Classifier\n* CatBoost Classifier\n","metadata":{}},{"cell_type":"code","source":"# Stratified Cross Validation Splitting\n\nsss = StratifiedKFold(n_splits=5, random_state=42, shuffle=False)\n\nfor train_index, test_index in sss.split(X_raw,y_raw):\n    \n    print(\"Train:\", train_index, \"Test:\", test_index)\n    X_train_sm, X_val_sm = X_raw.iloc[train_index], X_raw.iloc[test_index]\n    y_train_sm, y_val_sm = y_raw.iloc[train_index], y_raw.iloc[test_index]\n\n# Check the Distribution of the labels\n\n\n# Turn into an array\nX_train_sm = X_train_sm.values\nX_val_sm = X_val_sm.values\ny_train_sm = y_train_sm.values\ny_val_sm = y_val_sm.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(y_train_sm, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(y_val_sm, return_counts=True)\nprint('-' * 84)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label/ len(y_train_sm))\nprint(test_counts_label/ len(y_val_sm))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LOGISTIC REGRESSION ","metadata":{}},{"cell_type":"code","source":"# List to append the score and then find the average\n\naccuracy_lst_reg = []\nprecision_lst_reg = []\nrecall_lst_reg = []\nf1_lst_reg = []\nauc_lst_reg = []\n\nlog_reg_sm = LogisticRegression()\n#log_reg_params = {}\nlog_reg_params = {\"penalty\": ['l2'],\n                  'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n                  'class_weight': ['balanced',None],\n                  'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n\nfor train, val in sss.split(X_train_sm, y_train_sm):\n    pipeline_reg = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n    model_reg = pipeline_reg.fit(X_train_sm[train], y_train_sm[train])\n    best_est_reg = rand_log_reg.best_estimator_\n    prediction_reg = best_est_reg.predict(X_train_sm[val])\n    \n    accuracy_lst_reg.append(pipeline_reg.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_reg.append(precision_score(y_train_sm[val], prediction_reg))\n    recall_lst_reg.append(recall_score(y_train_sm[val], prediction_reg))\n    f1_lst_reg.append(f1_score(y_train_sm[val], prediction_reg))\n    auc_lst_reg.append(roc_auc_score(y_train_sm[val], prediction_reg))\n\n\nprint('---' * 45)\nprint('')\nprint('Logistic Regression (SMOTE) results:')\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_reg)))\nprint(\"precision: {}\".format(np.mean(precision_lst_reg)))\nprint(\"recall: {}\".format(np.mean(recall_lst_reg)))\nprint(\"f1: {}\".format(np.mean(f1_lst_reg)))\nprint('')\nprint('---' * 45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing the classification report\n\nlabel = ['Fin.Stable', 'Fin.Unstable']\npred_reg_sm = best_est_reg.predict(X_val_sm)\nprint(classification_report(y_val_sm, pred_reg_sm, target_names=label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Precision-Recall curve\n\ny_score_reg = best_est_reg.predict(X_val_sm)\n\naverage_precision = average_precision_score(y_val_sm, y_score_reg)\n\nfig = plt.figure(figsize=(12,6))\n\nprecision, recall, _ = precision_recall_curve(y_val_sm, y_score_reg)\n\nplt.step(recall, precision, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision), fontsize=15)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RANDOM FOREST CLASSIFIER","metadata":{}},{"cell_type":"code","source":"# List to append the score and then find the average\n\naccuracy_lst_rfc = []\nprecision_lst_rfc = []\nrecall_lst_rfc = []\nf1_lst_rfc = []\nauc_lst_rfc = []\n\nrfc_sm = RandomForestClassifier()\n#rfc_params = {}\nrfc_params = {'max_features' : ['auto', 'sqrt', 'log2'],\n              'random_state' : [42],\n              'class_weight' : ['balanced','balanced_subsample'],\n              'criterion' : ['gini', 'entropy'],\n              'bootstrap' : [True,False]}\n    \n    \nrand_rfc = RandomizedSearchCV(rfc_sm, rfc_params, n_iter=4)\n\n\nfor train, val in sss.split(X_train_sm, y_train_sm):\n    pipeline_rfc = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_rfc) # SMOTE happens during Cross Validation not before..\n    model_rfc = pipeline_rfc.fit(X_train_sm, y_train_sm)\n    best_est_rfc = rand_rfc.best_estimator_\n    prediction_rfc = best_est_rfc.predict(X_train_sm[val])\n    \n    accuracy_lst_rfc.append(pipeline_rfc.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_rfc.append(precision_score(y_train_sm[val], prediction_rfc))\n    recall_lst_rfc.append(recall_score(y_train_sm[val], prediction_rfc))\n    f1_lst_rfc.append(f1_score(y_train_sm[val], prediction_rfc))\n    auc_lst_rfc.append(roc_auc_score(y_train_sm[val], prediction_rfc))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_rfc)))\nprint(\"precision: {}\".format(np.mean(precision_lst_rfc)))\nprint(\"recall: {}\".format(np.mean(recall_lst_rfc)))\nprint(\"f1: {}\".format(np.mean(f1_lst_rfc)))\nprint('---' * 45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smote_prediction_rfc = best_est_rfc.predict(X_val_sm)\nprint(classification_report(y_val_sm, smote_prediction_rfc, target_names=label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Precision-Recall curve\n\ny_score_rfc = best_est_rfc.predict(X_val_sm)\n\naverage_precision_rfc = average_precision_score(y_val_sm, y_score_rfc)\n\nfig = plt.figure(figsize=(12,6))\n\nprecision_rfc, recall_rfc, _ = precision_recall_curve(y_val_sm, y_score_rfc)\n\nplt.step(recall_rfc, precision_rfc, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall_rfc, precision_rfc, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision_rfc), fontsize=15)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBOOST","metadata":{}},{"cell_type":"code","source":"# List to append the score and then find the average\n\naccuracy_lst_xgb = []\nprecision_lst_xgb = []\nrecall_lst_xgb = []\nf1_lst_xgb = []\nauc_lst_xgb = []\n\nxgb_sm = xgb.XGBClassifier(random_state = 42)\nxgb_params = {'eta' : [0.1,0.01,0.001],  # Learning rate\n              'eval_metric': ['logloss'],\n              'max_depth' : [3,6,9],\n              'lambda' : [1,1.5,2],      # L2 regularization (higher values make model more conservative)\n              'alpha' : [0,0.5,1]}        # L1 regularization (higher values make model more conservative)\n              #'reg' : ['squarederror']}\n              #'random_state': [42]}\n        \nrand_xgb = RandomizedSearchCV(xgb_sm, xgb_params, n_iter=4)\n\n\nfor train, val in sss.split(X_train_sm, y_train_sm):\n    pipeline_xgb = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_xgb) # SMOTE happens during Cross Validation not before..\n    model_xgb = pipeline_xgb.fit(X_train_sm, y_train_sm)\n    best_est_xgb = rand_xgb.best_estimator_\n    prediction_xgb = best_est_xgb.predict(X_train_sm[val])\n    \n    accuracy_lst_xgb.append(pipeline_xgb.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_xgb.append(precision_score(y_train_sm[val], prediction_xgb))\n    recall_lst_xgb.append(recall_score(y_train_sm[val], prediction_xgb))\n    f1_lst_xgb.append(f1_score(y_train_sm[val], prediction_xgb))\n    auc_lst_xgb.append(roc_auc_score(y_train_sm[val], prediction_xgb))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_xgb)))\nprint(\"precision: {}\".format(np.mean(precision_lst_xgb)))\nprint(\"recall: {}\".format(np.mean(recall_lst_xgb)))\nprint(\"f1: {}\".format(np.mean(f1_lst_xgb)))\nprint('---' * 45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing classification report\n\nsmote_prediction_xgb = best_est_xgb.predict(X_val_sm)\nprint(classification_report(y_val_sm, smote_prediction_xgb, target_names=label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Precision-Recall curve\n\ny_score_xgb = best_est_xgb.predict(X_val_sm)\n\naverage_precision_xgb = average_precision_score(y_val_sm, y_score_xgb)\n\nfig = plt.figure(figsize=(12,6))\n\nprecision_xgb, recall_xgb, _ = precision_recall_curve(y_val_sm, y_score_xgb)\n\nplt.step(recall_xgb, precision_xgb, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall_xgb, precision_xgb, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision_xgb), fontsize=15)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CATBOOST","metadata":{}},{"cell_type":"code","source":"# List to append the score and then find the average\n\naccuracy_lst_cat = []\nprecision_lst_cat = []\nrecall_lst_cat = []\nf1_lst_cat = []\nauc_lst_cat = []\n\ncat_sm = CatBoostClassifier(verbose = 0)\n\ncat_params = {'eval_metric': ['F1'],\n              'iterations': [100,500,1000],\n              'learning_rate' : [0.1,0.01,0.001],\n              'random_seed' : [42],\n              'auto_class_weights' : ['Balanced','SqrtBalanced']\n             }\n    \n    \nrand_cat = RandomizedSearchCV(cat_sm, cat_params, n_iter=4)\n\n\nfor train, val in sss.split(X_train_sm, y_train_sm):\n    pipeline_cat = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_cat) # SMOTE happens during Cross Validation not before..\n    model_cat = pipeline_cat.fit(X_train_sm, y_train_sm)\n    best_est_cat = rand_cat.best_estimator_\n    prediction_cat = best_est_cat.predict(X_train_sm[val])\n    \n    accuracy_lst_cat.append(pipeline_cat.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_cat.append(precision_score(y_train_sm[val], prediction_cat))\n    recall_lst_cat.append(recall_score(y_train_sm[val], prediction_cat))\n    f1_lst_cat.append(f1_score(y_train_sm[val], prediction_cat))\n    auc_lst_cat.append(roc_auc_score(y_train_sm[val], prediction_cat))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_cat)))\nprint(\"precision: {}\".format(np.mean(precision_lst_cat)))\nprint(\"recall: {}\".format(np.mean(recall_lst_cat)))\nprint(\"f1: {}\".format(np.mean(f1_lst_cat)))\nprint('---' * 45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smote_prediction_cat = best_est_cat.predict(X_val_sm)\nprint(classification_report(y_val_sm, smote_prediction_cat, target_names=label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Precision-Recall curve\n\nsmote_prediction_cat = best_est_cat.predict(X_val_sm)\n\naverage_precision_cat = average_precision_score(y_val_sm, smote_prediction_cat)\n\nfig = plt.figure(figsize=(12,6))\n\nprecision_cat, recall_cat, _ = precision_recall_curve(y_val_sm, smote_prediction_cat)\n\nplt.step(recall_cat, precision_cat, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall_cat, precision_cat, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision_cat), fontsize=15)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RESULTS","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nlog_fpr, log_tpr, log_thresold = roc_curve(y_val_sm, pred_reg_sm)\nrfc_fpr, rfc_tpr, rfc_threshold = roc_curve(y_val_sm, smote_prediction_rfc)\nxgb_fpr, xgb_tpr, xgb_thresold = roc_curve(y_val_sm, smote_prediction_xgb)\ncat_fpr, cat_tpr, cat_thresold = roc_curve(y_val_sm, smote_prediction_cat)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, rfc_fpr, rfc_tpr,xgb_fpr, xgb_tpr,cat_fpr, cat_tpr):\n    plt.figure(figsize=(20,8))\n    plt.title('ROC Curve', fontsize=14)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, pred_reg_sm)))\n    plt.plot(xgb_fpr, xgb_tpr, label='XGBoost Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, smote_prediction_xgb)))\n    plt.plot(cat_fpr, cat_tpr, label='CatBoost Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, smote_prediction_cat)))\n    plt.plot(rfc_fpr, rfc_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, smote_prediction_rfc)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=13)\n    plt.ylabel('True Positive Rate', fontsize=13)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, rfc_fpr, rfc_tpr,xgb_fpr, xgb_tpr,cat_fpr, cat_tpr)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting confusion matrix for each classifier\n\nconf_mx0 = confusion_matrix(y_val_sm,pred_reg_sm)\nconf_mx1 = confusion_matrix(y_val_sm,smote_prediction_rfc)\nconf_mx2 = confusion_matrix(y_val_sm,smote_prediction_xgb)\nconf_mx3 = confusion_matrix(y_val_sm,smote_prediction_cat)\n\nheat_cm0 = pd.DataFrame(conf_mx0, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\nheat_cm0.index.name = 'Actual'\nheat_cm0.columns.name = 'Predicted'\n\nheat_cm1 = pd.DataFrame(conf_mx1, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\nheat_cm1.index.name = 'Actual'\nheat_cm1.columns.name = 'Predicted'\n\nheat_cm2 = pd.DataFrame(conf_mx2, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\nheat_cm2.index.name = 'Actual'\nheat_cm2.columns.name = 'Predicted'\n\nheat_cm3 = pd.DataFrame(conf_mx3, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\nheat_cm3.index.name = 'Actual'\nheat_cm3.columns.name = 'Predicted'\n\nf, ax = plt.subplots(1, 4, figsize=(20,8))\nf.subplots_adjust(left=None, bottom=None, right= 2, top=None, wspace=None, hspace= None)\n\nsns.heatmap(heat_cm0, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[0])\nax[0].set_title('Logistic Regression', fontsize = 20)\nsns.heatmap(heat_cm1, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[1])\nax[1].set_title('Random Forest Classifier', fontsize = 20)\nsns.heatmap(heat_cm2, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[2])\nax[2].set_title('XGBoost Classifier', fontsize = 20)\nsns.heatmap(heat_cm3, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[3])\nax[3].set_title('Catboot Classifier', fontsize = 20)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"검증 세트의 결과에서 볼 수 있듯이, 모든 모델은 파산에 가까운 회사를 탐지하는 데 여전히 문제가 있습니다. 소수 집단에 대한 더 많은 관찰을 인식하는 알고리즘은 로지스틱 회귀이지만, 정밀도 측면에서 큰 비용이 듭니다(허위 음수가 많이 존재함). 오류가 존재함에도 불구하고 이 경우 파산에 가까운 관찰이 아닌 것을 그 반대의 경우보다 파산에 가까운 것으로 식별하는 것이 더 낫다고 생각하므로 유용한 모델이 될 수 있습니다.","metadata":{}},{"cell_type":"code","source":"# Testing\n\ntest_pred_lr = best_est_reg.predict(X_test)\n#test_pred_rf = best_est_rfc.predict(X_test)\n#test_pred_xgb = best_est_xgb.predict(X_test) \ntest_pred_cat = best_est_cat.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting confusion matrix for each classifier\n\nconf_mx0 = confusion_matrix(y_test,test_pred_lr)\nconf_mx1 = confusion_matrix(y_test,test_pred_cat)\n\nheat_cm0 = pd.DataFrame(conf_mx0, columns=np.unique(y_test), index = np.unique(y_test))\nheat_cm0.index.name = 'Actual'\nheat_cm0.columns.name = 'Predicted'\n\nheat_cm1 = pd.DataFrame(conf_mx1, columns=np.unique(y_test), index = np.unique(y_test))\nheat_cm1.index.name = 'Actual'\nheat_cm1.columns.name = 'Predicted'\n\nf, ax = plt.subplots(1, 2, figsize=(15,8))\nf.subplots_adjust(left=None, bottom=None, right= 2, top=None, wspace=None, hspace= None)\n\nsns.heatmap(heat_cm0, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[0])\nax[0].set_title('Logistic Regression', fontsize = 20)\nsns.heatmap(heat_cm1, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[1])\nax[1].set_title('Catboot Classifier', fontsize = 20)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, test_pred_lr, target_names=label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, test_pred_cat, target_names=label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"검증 데이터를 사용하여 표시된 것과 정확히 일치하게 캣부스트를 사용하면 고려된 지표(F1)가 어떻게 더 높은지 알 수 있습니다. 그럼에도 불구하고 이 경우 소수자 계층이 파산에 근접하지 않은 일부 기업을 파산에 근접한 것으로 잘못 분류하는 것을 더 잘 인식할 수 있기 때문에 로지스틱 회귀를 사용하는 것이 최선의 결정입니다.\n\n지원해 주셔서 감사드리며 다음 노트북으로 뵙겠습니다!","metadata":{}}]}