{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":1938459,"sourceType":"datasetVersion","datasetId":1111894}],"dockerImageVersionId":30055,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## íšŒì‚¬ íŒŒì—… ğŸ“‰ğŸ’¸\nëŒ€ë§Œ ì¦ê¶Œê±°ë˜ì†Œì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì •ì— ë”°ë¼ íšŒì‚¬ íŒŒì‚°ì„ ë‚˜íƒ€ë‚´ëŠ” 1999~2009ë…„ ëŒ€ë§Œ ê²½ì œ ì €ë„ì˜ ë°ì´í„°ë‹¤.\n\nì†Œê°œ\n\nì´ ì»¤ë„ì—ì„œëŠ” ë‹¤ì–‘í•œ ì˜ˆì¸¡ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í–¥í›„ íŒŒì‚°ì— ì§ë©´í•  ê¸°ì—…ì„ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ê°ì§€í•˜ëŠ” ë° ì–¼ë§ˆë‚˜ ì •í™•í•œì§€ í™•ì¸í•  ê²ƒì…ë‹ˆë‹¤. ë°ì´í„° ì„¹ì…˜ì— ì„¤ëª…ëœ ëŒ€ë¡œ ë°ì´í„° ì„¸íŠ¸ì—ëŠ” ë‹¤ìŒì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:\n\n95ê°œ ê¸°ëŠ¥(X1-X95, ëŒ€ë§Œ ì¦ê¶Œê±°ë˜ì†Œ ì—…ë¬´ ê·œì •)\n1 ë¼ë²¨ ë²¡í„°\nì´ í”„ë¡œì íŠ¸ì˜ ëª©í‘œëŠ” ì´ëŸ¬í•œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ì„ íƒëœ ëª¨ë¸ì— ë¯¸ì¹˜ëŠ” ì˜í–¥/ì—­í• ì„ íŒŒì•…í•˜ê³  ë±…í¬ëŸ½ì— ê°€ê¹Œìš´ ê¸°ì—…ì„ ì¸ì‹í•˜ëŠ” ë° ì–´ë–»ê²Œ ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ”ì§€ íŒŒì•…í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì‹œì‘í•©ë‹ˆë‹¤!\n\nì¶”ì‹ : ì´ ë…¸íŠ¸ë¶ì´ ë§ˆìŒì— ë“œì‹ ë‹¤ë©´ UPVOTEë¥¼ ìŠì§€ ë§ˆì„¸ìš”!","metadata":{}},{"cell_type":"code","source":"# IMPORTING LIBRARIES\n\n# General Libraries\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n\n# Preprocessing Libraries\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Machine Learning Libraries\n\nimport sklearn\nimport xgboost as xgb\nfrom sklearn import tree\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_curve\nfrom imblearn.pipeline import Pipeline\nfrom catboost import CatBoostClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.metrics import classification_report\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import KFold, cross_validate\nfrom sklearn.metrics import recall_score, f1_score, roc_auc_score\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom sklearn.metrics import precision_recall_curve, average_precision_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n\n\n\n# Defining the working directory\n\ninput_path = '../input/company-bankruptcy-prediction/'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# IMPORTING DATA\n\nbank_data = pd.read_csv(input_path + 'data.csv')\nbank_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bank_data.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ì´ì œ ë°ì´í„°ì— ëŒ€í•œ ì•„ì´ë””ì–´ê°€ ìƒê²¼ìœ¼ë‹ˆ ë°ì´í„°ì— ëŒ€í•œ ë” ë§ì€ ì •ë³´ë¥¼ ì–»ì–´ì•¼ í•©ë‹ˆë‹¤. ê°€ì¥ ë¨¼ì € ì´í•´í•˜ê³  ì‹¶ì€ ê²ƒì€ ë°ì´í„°ì˜ íŠ¹ì„±, ì¦‰ ë°ì´í„°ê°€ ìˆ˜ì¹˜ ë˜ëŠ” ë²”ì£¼í˜•ì¸ì§€, ê·¸ë¦¬ê³  ê·¸ ì¤‘ì— ëˆ„ë½ëœ ì •ë³´ê°€ ìˆëŠ”ì§€ ì—¬ë¶€ì…ë‹ˆë‹¤. info() ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ ë‘ ê°€ì§€ ì ì„ ëª¨ë‘ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nê²°ê³¼ íŒ¨ë„ì€ ìš°ë¦¬ì—ê²Œ ê°•ë ¥í•œ ì •ë³´ë¥¼ ì œê³µí•˜ë©°, ê·¸ ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:\n\në°ì´í„° ì„¸íŠ¸ëŠ” 96ê°œì˜ features ê°ê°ì— ëŒ€í•´ 6819ê°œì˜ ê´€ì°° ê²°ê³¼ ì¡°í•©ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ëª¨ë“  featuresì€ ìˆ«ì(int64 ë˜ëŠ” float64)ì…ë‹ˆë‹¤ ë°ì´í„° ì¤‘ ëˆ„ë½ëœ ê°’(Nan)ì´ ì—†ìŠµë‹ˆë‹¤ ëª¨ë“  featuresê°€ ìˆ«ìì´ê¸° ë•Œë¬¸ì— ì¶”ê°€ ì •ë³´ ì¶œì²˜ì¸ ê¸°ìˆ  í†µê³„ë¥¼ ì‰½ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.","metadata":{}},{"cell_type":"markdown","source":"ê²°ê³¼ íŒ¨ë„ì€ ìš°ë¦¬ì—ê²Œ ê°•ë ¥í•œ ì •ë³´ë¥¼ ì œê³µí•˜ë©°, ê·¸ ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:\n\në°ì´í„° ì„¸íŠ¸ëŠ” 96ê°œì˜ ê¸°ëŠ¥ ê°ê°ì— ëŒ€í•´ 6819ê°œì˜ ê´€ì°° ê²°ê³¼ ì¡°í•©ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.\nëª¨ë“  ê¸°ëŠ¥ì€ ìˆ«ì(int64 ë˜ëŠ” float64)ì…ë‹ˆë‹¤\në°ì´í„° ì¤‘ ëˆ„ë½ëœ ê°’(Nan)ì´ ì—†ìŠµë‹ˆë‹¤\nëª¨ë“  ê¸°ëŠ¥ì´ ìˆ«ìì´ê¸° ë•Œë¬¸ì— ì¶”ê°€ ì •ë³´ ì¶œì²˜ì¸ ê¸°ìˆ  í†µê³„ë¥¼ ì‰½ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.","metadata":{}},{"cell_type":"code","source":"# Computing the descriptive statistics of our numrerical features\n\nbank_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ëˆ„ë½ëœ ê°’ì´ ì—†ë‹¤ëŠ” ê²ƒì„ ì´ë¯¸ ì•Œê³  ìˆì§€ë§Œ(ì—¬ê¸°ì„œëŠ” 96ê°œì˜ featureë§Œ ìˆë‹¤ëŠ” ì ì„ ê³ ë ¤í•˜ë©´ ë§¤ìš° ì‰½ìŠµë‹ˆë‹¤), í”„ë¡œì íŠ¸ì˜ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ì™€ ì‹œê°„ ë‚­ë¹„ë¥¼ ë°©ì§€í•˜ë ¤ë©´ ì´ê²ƒì´ ì‚¬ì‹¤ì¸ì§€ ê³„ì‚°ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤.","metadata":{}},{"cell_type":"code","source":"# Checking Nan presence\n\n#bank_data.isna().sum().max()\n[print(col) for col in bank_data if bank_data[col].isna().sum() > 0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ë°ì´í„°ì— ì¤‘ë³µì´ ìˆì„ ê°€ëŠ¥ì„±ì— ëŒ€í•´ ì¶”ê°€ë¡œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ì¤‘ë³µì€ ë°ì´í„°ì— ì¤‘ë³µì„ ì¼ìœ¼í‚¬ ìˆ˜ ìˆëŠ” ë™ì¼í•œ ê´€ì°°ì´ë¯€ë¡œ ì‚­ì œí•´ì•¼ í•©ë‹ˆë‹¤. ì œê±°í•  ì¤‘ë³µì´ ìˆëŠ”ì§€ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤:","metadata":{}},{"cell_type":"code","source":"# Checking for duplicates\n\nbank_data.duplicated().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ë°ì´í„°ì— ì¤‘ë³µì´ ìˆì„ ê°€ëŠ¥ì„±ì— ëŒ€í•´ ì¶”ê°€ë¡œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ì¤‘ë³µì€ ë°ì´í„°ì— ì¤‘ë³µì„ ì¼ìœ¼í‚¬ ìˆ˜ ìˆëŠ” ë™ì¼í•œ ê´€ì°°ì´ë¯€ë¡œ ì‚­ì œí•´ì•¼ í•©ë‹ˆë‹¤. ì œê±°í•  ì¤‘ë³µì´ ìˆëŠ”ì§€ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤:","metadata":{}},{"cell_type":"code","source":"# The classes are heavily skewed we need to solve this issue later.\n\nprint(bank_data['Bankrupt?'].value_counts())\nprint('-'* 30)\nprint('Financially stable: ', round(bank_data['Bankrupt?'].value_counts()[0]/len(bank_data) * 100,2), '% of the dataset')\nprint('Financially unstable: ', round(bank_data['Bankrupt?'].value_counts()[1]/len(bank_data) * 100,2), '% of the dataset')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking labels distributions\n\nsns.set_theme(context = 'paper')\n\nplt.figure(figsize = (10,5))\nsns.countplot(bank_data['Bankrupt?'])\nplt.title('Class Distributions \\n (0: Fin. Stable || 1: Fin. Unstable)', fontsize=14)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ìœ„ì˜ í”Œë¡¯ì„ ë³´ë©´ ë ˆì´ë¸”ì˜ ë¶ˆê· í˜•ì´ ì–¼ë§ˆë‚˜ ì‹¬í•œì§€ ëª…í™•í•˜ê²Œ ì•Œ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ê¸° ìœ„í•´ í•´ê²°í•´ì•¼ í•  ì£¼ìš” ì¥ì• ë¬¼ì…ë‹ˆë‹¤.","metadata":{}},{"cell_type":"markdown","source":"Exploratory Data Analysis (EDA).\n\nì—¬ëŸ¬ ìœ í˜•ì˜ ì‹œê°í™”ì—ì„œ ê°€ëŠ¥í•œ í•œ ë§ì€ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚´í´ë´…ì‹œë‹¤. ê°€ì¥ ë¨¼ì € ë³´ì—¬ì¤„ ì¼ë°˜ì ì¸ í”Œë¡¯ì€ ë°ì´í„° ì„¸íŠ¸ì˜ ìˆ˜ì¹˜ ë³€ìˆ˜ ê°„ì˜ ëª¨ë“  ìƒê´€ê´€ê³„(ì„ íƒí•œ í…ŒìŠ¤íŠ¸ì— ë”°ë¼ ì„ í˜• ë° ë¹„ ì„ í˜•)ë¥¼ í”Œë¡¯í•˜ëŠ” ìƒê´€ê´€ê³„ íˆíŠ¸ë§µì…ë‹ˆë‹¤: í”¼ì–´ìŠ¨ ë˜ëŠ” ìŠ¤í”¼ì–´ë§¨. ê¸°ëŠ¥ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì€ í›ˆë ¨ ì¤‘ì— ìœ ì§€í•´ì•¼ í•  ê¸°ëŠ¥ì„ ê²°ì •í•˜ëŠ” ë° ì¤‘ìš”í•œ ë‹¨ê³„ì…ë‹ˆë‹¤.","metadata":{}},{"cell_type":"code","source":"# Looking at the histograms of numerical data\n\nbank_data.hist(figsize = (35,30), bins = 50)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EDA & VISUALIZATIONS\n\n# Correlation Heatmap (Spearman)\n\nf, ax = plt.subplots(figsize=(30, 25))\nmat = bank_data.corr('spearman')\nmask = np.triu(np.ones_like(mat, dtype=bool))\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\nsns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0,# annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Boxplots of the numerical features\n\nplt.figure(figsize = (20,20))\nax =sns.boxplot(data = bank_data, orient=\"h\")\nax.set_title('Bank Data Boxplots', fontsize = 18)\nax.set(xscale=\"log\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting interesting features\n\nf, axes = plt.subplots(ncols=4, figsize=(24,6))\n\nsns.boxplot(x='Bankrupt?', y=\" Net Income to Total Assets\", data=bank_data, ax=axes[0])\naxes[0].set_title('Bankrupt vs Net Income to Total Assets')\n\nsns.boxplot(x='Bankrupt?', y=\" Total debt/Total net worth\", data=bank_data, ax=axes[1]) \naxes[1].set_title('Bankrupt vs Tot Debt/Net worth Correlation')\n\n\nsns.boxplot(x='Bankrupt?', y=\" Debt ratio %\", data=bank_data, ax=axes[2])\naxes[2].set_title('Bankrupt vs Debt ratio Correlation')\n\n\nsns.boxplot(x='Bankrupt?', y=\" Net worth/Assets\", data=bank_data, ax=axes[3])  \naxes[3].set_title('Bankrupt vs Net Worth/Assets Correlation') \n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting other interesting features\n\nf, axes = plt.subplots(ncols=4, figsize=(24,6))\n\nsns.boxplot(x='Bankrupt?', y=\" Working Capital to Total Assets\", data=bank_data, ax=axes[0])\naxes[0].set_title('Bankrupt vs  working capital to total assets')\n\nsns.boxplot(x='Bankrupt?', y=\" Cash/Total Assets\", data=bank_data, ax=axes[1])\naxes[1].set_title('Bankrupt vs cash / total assets')\n\n\nsns.boxplot(x='Bankrupt?', y=\" Current Liability to Assets\", data=bank_data, ax=axes[2])\naxes[2].set_title('Bankrupt vs current liability to assets')\n\n\nsns.boxplot(x='Bankrupt?', y=\" Retained Earnings to Total Assets\", data=bank_data, ax=axes[3])\naxes[3].set_title('Bankrupt vs  Retained Earnings to Total Assets')\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ì´ì œ íŒŒì‚°ì´ ì„ë°•í•œ ê¸°ì—…ì— ëŒ€í•œ ì´ëŸ¬í•œ ê¸°ëŠ¥ì˜ ë¶„í¬ë¥¼ ì‚´í´ë´…ì‹œë‹¤:","metadata":{}},{"cell_type":"code","source":"# Plotting the feature distributions for close to bankrputcy companies\n\nf, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n\ncash_flow_rate = bank_data[' Net Income to Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(cash_flow_rate,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title(' Net Income to Total Assets \\n (Unstable companies)', fontsize=14)\n\ntot_debt_net = bank_data[' Total debt/Total net worth'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(tot_debt_net ,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('total debt/tot net worth \\n (Unstable companies)', fontsize=14)\n\n\ndebt_ratio = bank_data[' Debt ratio %'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(debt_ratio,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('debt_ratio \\n (Unstable companies)', fontsize=14)\n\nnet_worth_assets = bank_data[' Net worth/Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(net_worth_assets,ax=ax4, fit=norm, color='#C5B3F9')\nax4.set_title('net worth/assets \\n (Unstable companies)', fontsize=14)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n\nworking_cap = bank_data[' Working Capital to Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(working_cap,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('working capitals to total assets \\n (Unstable companies)', fontsize=14)\n\ncash_tot_assets = bank_data[' Cash/Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(cash_tot_assets ,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('cash/total assets \\n (Unstable companies)', fontsize=14)\n\n\nasset_liab = bank_data[' Current Liability to Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(asset_liab,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('liability to assets \\n (Unstable companies)', fontsize=14)\n\noperating_funds = bank_data[' Retained Earnings to Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\nsns.distplot(operating_funds,ax=ax4, fit=norm, color='#C5B3F9')\nax4.set_title('retain earnings to total assets \\n (Unstable companies)', fontsize=14)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ì´ìƒê°’ ì œê±°\n\nì´ í”„ë¡œì íŠ¸ì—ì„œëŠ” ê°€ì¥ ê·¹ë‹¨ì ì¸ ì´ìƒê°’ì„ ì œê±°í•˜ë ¤ê³  í•©ë‹ˆë‹¤(ì´ë¥¼ ì œê±°í•˜ëŠ” ëŒ€ì‹  í‰ê·  ë˜ëŠ” ì¤‘ì•™ê°’ìœ¼ë¡œ ì¶”ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤). ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ í–¥ìƒë  ê²ƒì…ë‹ˆë‹¤.","metadata":{}},{"cell_type":"code","source":"# Outliers removal\n\ndef outliers_removal(feature,feature_name,dataset):\n    \n    # Identify 25th & 75th quartiles\n\n    q25, q75 = np.percentile(feature, 25), np.percentile(feature, 75)\n    print('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n    feat_iqr = q75 - q25\n    print('iqr: {}'.format(feat_iqr))\n    \n    feat_cut_off = feat_iqr * 1.5\n    feat_lower, feat_upper = q25 - feat_cut_off, q75 + feat_cut_off\n    print('Cut Off: {}'.format(feat_cut_off))\n    print(feature_name +' Lower: {}'.format(feat_lower))\n    print(feature_name +' Upper: {}'.format(feat_upper))\n    \n    outliers = [x for x in feature if x < feat_lower or x > feat_upper]\n    print(feature_name + ' outliers for close to bankruptcy cases: {}'.format(len(outliers)))\n    #print(feature_name + ' outliers:{}'.format(outliers))\n\n    dataset = dataset.drop(dataset[(dataset[feature_name] > feat_upper) | (dataset[feature_name] < feat_lower)].index)\n    print('-' * 65)\n    \n    return dataset\n\nfor col in bank_data:\n    new_df = outliers_removal(bank_data[col],str(col),bank_data)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ì´ì œ ì²­ì†Œëœ ìƒì ê·¸ë¦¼ì„ ì‚´í´ë´…ì‹œë‹¤:","metadata":{}},{"cell_type":"code","source":"f,(ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24,6))\n\n# Boxplots with outliers removed\n\nsns.boxplot(x=\"Bankrupt?\", y=\" Net Income to Total Assets\", data=new_df,ax=ax1) \nax1.set_title(\"Net Income to Total Assets \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=\" Total debt/Total net worth\", data=new_df,ax=ax2) \nax2.set_title(\"total debt/total net worth \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=\" Debt ratio %\", data=new_df,ax=ax3) \nax3.set_title(\"debt ratio % \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=' Net worth/Assets', data=new_df,ax=ax4) \nax4.set_title(\"net worth/assets \\n Reduction of outliers\", fontsize=14)\n        \nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f,(ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24,6))\n\n# Boxplots with outliers removed\n\nsns.boxplot(x=\"Bankrupt?\", y=' Working Capital to Total Assets', data=new_df,ax=ax1) \nax1.set_title(\"working capital to total assets \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=' Cash/Total Assets', data=new_df,ax=ax2) \nax2.set_title(\"cash / total assets \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=' Current Liability to Assets', data=new_df,ax=ax3) \nax3.set_title(\"current liability to assets \\n Reduction of outliers\", fontsize=14)\n\nsns.boxplot(x=\"Bankrupt?\", y=' Retained Earnings to Total Assets', data=new_df,ax=ax4) \nax4.set_title(\"Retained Earnings to Total Assets \\n Reduction of outliers\", fontsize=14)\n        \nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting the feature distributions for close to bankrputcy companies\n\nf, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n\ncash_flow_rate = new_df[' Net Income to Total Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(cash_flow_rate,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title(' Net Income to Total Assets \\n (Unstable companies)', fontsize=14)\n\ntot_debt_net = new_df[' Total debt/Total net worth'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(tot_debt_net ,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('total debt/tot net worth \\n (Unstable companies)', fontsize=14)\n\n\ndebt_ratio = new_df[' Debt ratio %'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(debt_ratio,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('debt_ratio \\n (Unstable companies)', fontsize=14)\n\nnet_worth_assets = new_df[' Net worth/Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(net_worth_assets,ax=ax4, fit=norm, color='#C5B3F9')\nax4.set_title('net worth/assets \\n (Unstable companies)', fontsize=14)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n\nworking_cap = new_df[' Working Capital to Total Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(working_cap,ax=ax1, fit=norm, color='#FB8861')\nax1.set_title('working capitals to total assets \\n (Unstable companies)', fontsize=14)\n\ncash_tot_assets = new_df[' Cash/Total Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(cash_tot_assets ,ax=ax2, fit=norm, color='#56F9BB')\nax2.set_title('cash/total assets \\n (Unstable companies)', fontsize=14)\n\n\nasset_liab = new_df[' Current Liability to Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(asset_liab,ax=ax3, fit=norm, color='#C5B3F9')\nax3.set_title('liability to assets \\n (Unstable companies)', fontsize=14)\n\noperating_funds = new_df[' Retained Earnings to Total Assets'].loc[new_df['Bankrupt?'] == 1].values\nsns.distplot(operating_funds,ax=ax4, fit=norm, color='#C5B3F9')\nax4.set_title('retain earnings to total assets \\n (Unstable companies)', fontsize=14)\n\nplt.show()","metadata":{"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ìš°ë¦¬ê°€ ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì€ ê·¹ë‹¨ì ì¸ ì´ìƒê°’ì„ ì œê±°í•˜ë©´ ë” ë§ì€ \"ì¢… ëª¨ì–‘\" ë¶„í¬ë¥¼ ì–»ëŠ” ë° í™•ì‹¤íˆ ë„ì›€ì´ ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤! (ì ì–´ë„ í‘œì‹œëœ featuresì˜ ê²½ìš°)","metadata":{}},{"cell_type":"code","source":"# Dividing Data and Labels\n\nlabels = new_df['Bankrupt?']\nnew_df = new_df.drop(['Bankrupt?'], axis = 1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def log_trans(data):\n    \n    for col in data:\n        skew = data[col].skew()\n        if skew > 0.5 or skew < -0.5:\n            data[col] = np.log1p(data[col])\n        else:\n            continue\n            \n    return data\n\ndata_norm = log_trans(new_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Boxplots of the preprocessed numerical features\n\nplt.figure(figsize = (20,20))\nax =sns.boxplot(data = data_norm, orient=\"h\")\nax.set_title('Bank Data Preprocessed Boxplots', fontsize = 18)\nax.set(xscale=\"log\")\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data_norm.hist(figsize = (35,30),bins = 50)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting Train and Test Data\n\nX_raw,X_test,y_raw,y_test  = train_test_split(data_norm,\n                                              labels,\n                                              test_size=0.1,\n                                              stratify = labels,\n                                              random_state = 42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"SMOTE \nì˜¤ë²„ìƒ˜í”Œë§ ê¸°ë²•ìœ¼ë¡œ SMOTEë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ë¬´ì‘ìœ„ ì–¸ë”ìƒ˜í”Œë§ë³´ë‹¤ SMOTEê°€ ë” ì •í™•í•  ê°€ëŠ¥ì„±ì´ ë†’ì§€ë§Œ, ì•ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼ í–‰ì´ ì œê±°ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— í›ˆë ¨í•˜ëŠ” ë° ë” ë§ì€ ì‹œê°„ì´ ê±¸ë¦´ ê²ƒì…ë‹ˆë‹¤.\n\nSMOTEì•Œê³ ë¦¬ì¦˜ì€ ì˜¤ë²„ìƒ˜í”Œë§ ê¸°ë²• ì¤‘ í•©ì„±ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ê³  ìˆëŠ” ëª¨ë¸ì´ë‹¤ SMOTE(synthetic minority oversampling technique)ë€, í•©ì„± ì†Œìˆ˜ ìƒ˜í”Œë§ ê¸°ìˆ ë¡œ ë‹¤ìˆ˜ í´ë˜ìŠ¤ë¥¼ ìƒ˜í”Œë§í•˜ê³  ê¸°ì¡´ ì†Œìˆ˜ ìƒ˜í”Œì„ ë³´ê°„í•˜ì—¬ ìƒˆë¡œìš´ ì†Œìˆ˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í•©ì„±í•´ë‚¸ë‹¤. ì¼ë°˜ì ì¸ ê²½ìš° ì„±ê³µì ìœ¼ë¡œ ì‘ë™í•˜ì§€ë§Œ, ì†Œìˆ˜ë°ì´í„°ë“¤ ì‚¬ì´ë¥¼ ë³´ê°„í•˜ì—¬ ì‘ë™í•˜ê¸° ë•Œë¬¸ì— ëª¨ë¸ë§ì…‹ì˜ ì†Œìˆ˜ë°ì´í„°ë“¤ ì‚¬ì´ì˜ íŠ¹ì„±ë§Œì„ ë°˜ì˜í•˜ê³  ìƒˆë¡œìš´ ì‚¬ë¡€ì˜ ë°ì´í„° ì˜ˆì¸¡ì—” ì·¨ì•½í•  ìˆ˜ ìˆë‹¤.\n\nSMOTE ë™ì‘ ë°©ì‹ ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ì´ë‚˜ KNN(ìµœê·¼ì ‘ì´ì›ƒ) ëª¨ë¸ ê¸°ë²•ì„ í™œìš©í•œë‹¤.\n\nì†Œìˆ˜ ë°ì´í„° ì¤‘ íŠ¹ì • ë²¡í„° (ìƒ˜í”Œ)ì™€ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•œë‹¤. ì´ ì°¨ì´ì— 0ê³¼ 1ì‚¬ì´ì˜ ë‚œìˆ˜ë¥¼ ê³±í•œë‹¤. íƒ€ê²Ÿ ë²¡í„°ì— ì¶”ê°€í•œë‹¤. ë‘ ê°œì˜ íŠ¹ì • ê¸°ëŠ¥ ì‚¬ì´ì˜ ì„ ë¶„ì„ ë”°ë¼ ì„ì˜ì˜ ì ì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤.","metadata":{}},{"cell_type":"markdown","source":"## MODELING\n\n\nì´ì œ ëª¨ë¸ë¡œ ë¬´ì—‡ì„ í•  ìˆ˜ ìˆëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤! ì •ë¦¬ëœ ì–¸ë”ìƒ˜í”Œë§ ë°ì´í„°ì™€ SMOTE ì˜¤ë²„ìƒ˜í”Œë§ ë°ì´í„°ì˜ ì„±ëŠ¥ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë¶€ë¶„ì„ ìœ„í•´ ì €ëŠ” ëª‡ ê°€ì§€ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸°ë¡œ ê²°ì •í–ˆìŠµë‹ˆë‹¤:\n\nLogistic Regression SVC Random Forest Classifier CatBoost Classifier\n\n* Logistic Regression\n* SVC \n* Random Forest Classifier\n* CatBoost Classifier\n","metadata":{}},{"cell_type":"code","source":"# Stratified Cross Validation Splitting\n\nsss = StratifiedKFold(n_splits=5, random_state=42, shuffle=False)\n\nfor train_index, test_index in sss.split(X_raw,y_raw):\n    \n    print(\"Train:\", train_index, \"Test:\", test_index)\n    X_train_sm, X_val_sm = X_raw.iloc[train_index], X_raw.iloc[test_index]\n    y_train_sm, y_val_sm = y_raw.iloc[train_index], y_raw.iloc[test_index]\n\n# Check the Distribution of the labels\n\n\n# Turn into an array\nX_train_sm = X_train_sm.values\nX_val_sm = X_val_sm.values\ny_train_sm = y_train_sm.values\ny_val_sm = y_val_sm.values\n\n# See if both the train and test label distribution are similarly distributed\ntrain_unique_label, train_counts_label = np.unique(y_train_sm, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(y_val_sm, return_counts=True)\nprint('-' * 84)\n\nprint('Label Distributions: \\n')\nprint(train_counts_label/ len(y_train_sm))\nprint(test_counts_label/ len(y_val_sm))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## LOGISTIC REGRESSION ","metadata":{}},{"cell_type":"code","source":"# List to append the score and then find the average\n\naccuracy_lst_reg = []\nprecision_lst_reg = []\nrecall_lst_reg = []\nf1_lst_reg = []\nauc_lst_reg = []\n\nlog_reg_sm = LogisticRegression()\n#log_reg_params = {}\nlog_reg_params = {\"penalty\": ['l2'],\n                  'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n                  'class_weight': ['balanced',None],\n                  'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n\nfor train, val in sss.split(X_train_sm, y_train_sm):\n    pipeline_reg = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n    model_reg = pipeline_reg.fit(X_train_sm[train], y_train_sm[train])\n    best_est_reg = rand_log_reg.best_estimator_\n    prediction_reg = best_est_reg.predict(X_train_sm[val])\n    \n    accuracy_lst_reg.append(pipeline_reg.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_reg.append(precision_score(y_train_sm[val], prediction_reg))\n    recall_lst_reg.append(recall_score(y_train_sm[val], prediction_reg))\n    f1_lst_reg.append(f1_score(y_train_sm[val], prediction_reg))\n    auc_lst_reg.append(roc_auc_score(y_train_sm[val], prediction_reg))\n\n\nprint('---' * 45)\nprint('')\nprint('Logistic Regression (SMOTE) results:')\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_reg)))\nprint(\"precision: {}\".format(np.mean(precision_lst_reg)))\nprint(\"recall: {}\".format(np.mean(recall_lst_reg)))\nprint(\"f1: {}\".format(np.mean(f1_lst_reg)))\nprint('')\nprint('---' * 45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing the classification report\n\nlabel = ['Fin.Stable', 'Fin.Unstable']\npred_reg_sm = best_est_reg.predict(X_val_sm)\nprint(classification_report(y_val_sm, pred_reg_sm, target_names=label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Precision-Recall curve\n\ny_score_reg = best_est_reg.predict(X_val_sm)\n\naverage_precision = average_precision_score(y_val_sm, y_score_reg)\n\nfig = plt.figure(figsize=(12,6))\n\nprecision, recall, _ = precision_recall_curve(y_val_sm, y_score_reg)\n\nplt.step(recall, precision, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision), fontsize=15)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RANDOM FOREST CLASSIFIER","metadata":{}},{"cell_type":"code","source":"# List to append the score and then find the average\n\naccuracy_lst_rfc = []\nprecision_lst_rfc = []\nrecall_lst_rfc = []\nf1_lst_rfc = []\nauc_lst_rfc = []\n\nrfc_sm = RandomForestClassifier()\n#rfc_params = {}\nrfc_params = {'max_features' : ['auto', 'sqrt', 'log2'],\n              'random_state' : [42],\n              'class_weight' : ['balanced','balanced_subsample'],\n              'criterion' : ['gini', 'entropy'],\n              'bootstrap' : [True,False]}\n    \n    \nrand_rfc = RandomizedSearchCV(rfc_sm, rfc_params, n_iter=4)\n\n\nfor train, val in sss.split(X_train_sm, y_train_sm):\n    pipeline_rfc = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_rfc) # SMOTE happens during Cross Validation not before..\n    model_rfc = pipeline_rfc.fit(X_train_sm, y_train_sm)\n    best_est_rfc = rand_rfc.best_estimator_\n    prediction_rfc = best_est_rfc.predict(X_train_sm[val])\n    \n    accuracy_lst_rfc.append(pipeline_rfc.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_rfc.append(precision_score(y_train_sm[val], prediction_rfc))\n    recall_lst_rfc.append(recall_score(y_train_sm[val], prediction_rfc))\n    f1_lst_rfc.append(f1_score(y_train_sm[val], prediction_rfc))\n    auc_lst_rfc.append(roc_auc_score(y_train_sm[val], prediction_rfc))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_rfc)))\nprint(\"precision: {}\".format(np.mean(precision_lst_rfc)))\nprint(\"recall: {}\".format(np.mean(recall_lst_rfc)))\nprint(\"f1: {}\".format(np.mean(f1_lst_rfc)))\nprint('---' * 45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smote_prediction_rfc = best_est_rfc.predict(X_val_sm)\nprint(classification_report(y_val_sm, smote_prediction_rfc, target_names=label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Precision-Recall curve\n\ny_score_rfc = best_est_rfc.predict(X_val_sm)\n\naverage_precision_rfc = average_precision_score(y_val_sm, y_score_rfc)\n\nfig = plt.figure(figsize=(12,6))\n\nprecision_rfc, recall_rfc, _ = precision_recall_curve(y_val_sm, y_score_rfc)\n\nplt.step(recall_rfc, precision_rfc, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall_rfc, precision_rfc, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision_rfc), fontsize=15)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XGBOOST","metadata":{}},{"cell_type":"code","source":"# List to append the score and then find the average\n\naccuracy_lst_xgb = []\nprecision_lst_xgb = []\nrecall_lst_xgb = []\nf1_lst_xgb = []\nauc_lst_xgb = []\n\nxgb_sm = xgb.XGBClassifier(random_state = 42)\nxgb_params = {'eta' : [0.1,0.01,0.001],  # Learning rate\n              'eval_metric': ['logloss'],\n              'max_depth' : [3,6,9],\n              'lambda' : [1,1.5,2],      # L2 regularization (higher values make model more conservative)\n              'alpha' : [0,0.5,1]}        # L1 regularization (higher values make model more conservative)\n              #'reg' : ['squarederror']}\n              #'random_state': [42]}\n        \nrand_xgb = RandomizedSearchCV(xgb_sm, xgb_params, n_iter=4)\n\n\nfor train, val in sss.split(X_train_sm, y_train_sm):\n    pipeline_xgb = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_xgb) # SMOTE happens during Cross Validation not before..\n    model_xgb = pipeline_xgb.fit(X_train_sm, y_train_sm)\n    best_est_xgb = rand_xgb.best_estimator_\n    prediction_xgb = best_est_xgb.predict(X_train_sm[val])\n    \n    accuracy_lst_xgb.append(pipeline_xgb.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_xgb.append(precision_score(y_train_sm[val], prediction_xgb))\n    recall_lst_xgb.append(recall_score(y_train_sm[val], prediction_xgb))\n    f1_lst_xgb.append(f1_score(y_train_sm[val], prediction_xgb))\n    auc_lst_xgb.append(roc_auc_score(y_train_sm[val], prediction_xgb))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_xgb)))\nprint(\"precision: {}\".format(np.mean(precision_lst_xgb)))\nprint(\"recall: {}\".format(np.mean(recall_lst_xgb)))\nprint(\"f1: {}\".format(np.mean(f1_lst_xgb)))\nprint('---' * 45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Printing classification report\n\nsmote_prediction_xgb = best_est_xgb.predict(X_val_sm)\nprint(classification_report(y_val_sm, smote_prediction_xgb, target_names=label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Precision-Recall curve\n\ny_score_xgb = best_est_xgb.predict(X_val_sm)\n\naverage_precision_xgb = average_precision_score(y_val_sm, y_score_xgb)\n\nfig = plt.figure(figsize=(12,6))\n\nprecision_xgb, recall_xgb, _ = precision_recall_curve(y_val_sm, y_score_xgb)\n\nplt.step(recall_xgb, precision_xgb, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall_xgb, precision_xgb, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision_xgb), fontsize=15)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CATBOOST","metadata":{}},{"cell_type":"code","source":"# List to append the score and then find the average\n\naccuracy_lst_cat = []\nprecision_lst_cat = []\nrecall_lst_cat = []\nf1_lst_cat = []\nauc_lst_cat = []\n\ncat_sm = CatBoostClassifier(verbose = 0)\n\ncat_params = {'eval_metric': ['F1'],\n              'iterations': [100,500,1000],\n              'learning_rate' : [0.1,0.01,0.001],\n              'random_seed' : [42],\n              'auto_class_weights' : ['Balanced','SqrtBalanced']\n             }\n    \n    \nrand_cat = RandomizedSearchCV(cat_sm, cat_params, n_iter=4)\n\n\nfor train, val in sss.split(X_train_sm, y_train_sm):\n    pipeline_cat = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_cat) # SMOTE happens during Cross Validation not before..\n    model_cat = pipeline_cat.fit(X_train_sm, y_train_sm)\n    best_est_cat = rand_cat.best_estimator_\n    prediction_cat = best_est_cat.predict(X_train_sm[val])\n    \n    accuracy_lst_cat.append(pipeline_cat.score(X_train_sm[val], y_train_sm[val]))\n    precision_lst_cat.append(precision_score(y_train_sm[val], prediction_cat))\n    recall_lst_cat.append(recall_score(y_train_sm[val], prediction_cat))\n    f1_lst_cat.append(f1_score(y_train_sm[val], prediction_cat))\n    auc_lst_cat.append(roc_auc_score(y_train_sm[val], prediction_cat))\n    \nprint('---' * 45)\nprint('')\nprint(\"accuracy: {}\".format(np.mean(accuracy_lst_cat)))\nprint(\"precision: {}\".format(np.mean(precision_lst_cat)))\nprint(\"recall: {}\".format(np.mean(recall_lst_cat)))\nprint(\"f1: {}\".format(np.mean(f1_lst_cat)))\nprint('---' * 45)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"smote_prediction_cat = best_est_cat.predict(X_val_sm)\nprint(classification_report(y_val_sm, smote_prediction_cat, target_names=label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting Precision-Recall curve\n\nsmote_prediction_cat = best_est_cat.predict(X_val_sm)\n\naverage_precision_cat = average_precision_score(y_val_sm, smote_prediction_cat)\n\nfig = plt.figure(figsize=(12,6))\n\nprecision_cat, recall_cat, _ = precision_recall_curve(y_val_sm, smote_prediction_cat)\n\nplt.step(recall_cat, precision_cat, color='r', alpha=0.2,\n         where='post')\nplt.fill_between(recall_cat, precision_cat, step='post', alpha=0.2,\n                 color='#F59B00')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.ylim([0.0, 1.05])\nplt.xlim([0.0, 1.0])\nplt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n          average_precision_cat), fontsize=15)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## RESULTS","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import roc_curve\n\nlog_fpr, log_tpr, log_thresold = roc_curve(y_val_sm, pred_reg_sm)\nrfc_fpr, rfc_tpr, rfc_threshold = roc_curve(y_val_sm, smote_prediction_rfc)\nxgb_fpr, xgb_tpr, xgb_thresold = roc_curve(y_val_sm, smote_prediction_xgb)\ncat_fpr, cat_tpr, cat_thresold = roc_curve(y_val_sm, smote_prediction_cat)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, rfc_fpr, rfc_tpr,xgb_fpr, xgb_tpr,cat_fpr, cat_tpr):\n    plt.figure(figsize=(20,8))\n    plt.title('ROC Curve', fontsize=14)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, pred_reg_sm)))\n    plt.plot(xgb_fpr, xgb_tpr, label='XGBoost Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, smote_prediction_xgb)))\n    plt.plot(cat_fpr, cat_tpr, label='CatBoost Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, smote_prediction_cat)))\n    plt.plot(rfc_fpr, rfc_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, smote_prediction_rfc)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=13)\n    plt.ylabel('True Positive Rate', fontsize=13)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, rfc_fpr, rfc_tpr,xgb_fpr, xgb_tpr,cat_fpr, cat_tpr)\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting confusion matrix for each classifier\n\nconf_mx0 = confusion_matrix(y_val_sm,pred_reg_sm)\nconf_mx1 = confusion_matrix(y_val_sm,smote_prediction_rfc)\nconf_mx2 = confusion_matrix(y_val_sm,smote_prediction_xgb)\nconf_mx3 = confusion_matrix(y_val_sm,smote_prediction_cat)\n\nheat_cm0 = pd.DataFrame(conf_mx0, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\nheat_cm0.index.name = 'Actual'\nheat_cm0.columns.name = 'Predicted'\n\nheat_cm1 = pd.DataFrame(conf_mx1, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\nheat_cm1.index.name = 'Actual'\nheat_cm1.columns.name = 'Predicted'\n\nheat_cm2 = pd.DataFrame(conf_mx2, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\nheat_cm2.index.name = 'Actual'\nheat_cm2.columns.name = 'Predicted'\n\nheat_cm3 = pd.DataFrame(conf_mx3, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\nheat_cm3.index.name = 'Actual'\nheat_cm3.columns.name = 'Predicted'\n\nf, ax = plt.subplots(1, 4, figsize=(20,8))\nf.subplots_adjust(left=None, bottom=None, right= 2, top=None, wspace=None, hspace= None)\n\nsns.heatmap(heat_cm0, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[0])\nax[0].set_title('Logistic Regression', fontsize = 20)\nsns.heatmap(heat_cm1, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[1])\nax[1].set_title('Random Forest Classifier', fontsize = 20)\nsns.heatmap(heat_cm2, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[2])\nax[2].set_title('XGBoost Classifier', fontsize = 20)\nsns.heatmap(heat_cm3, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[3])\nax[3].set_title('Catboot Classifier', fontsize = 20)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ê²€ì¦ ì„¸íŠ¸ì˜ ê²°ê³¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ëª¨ë“  ëª¨ë¸ì€ íŒŒì‚°ì— ê°€ê¹Œìš´ íšŒì‚¬ë¥¼ íƒì§€í•˜ëŠ” ë° ì—¬ì „íˆ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì†Œìˆ˜ ì§‘ë‹¨ì— ëŒ€í•œ ë” ë§ì€ ê´€ì°°ì„ ì¸ì‹í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì€ ë¡œì§€ìŠ¤í‹± íšŒê·€ì´ì§€ë§Œ, ì •ë°€ë„ ì¸¡ë©´ì—ì„œ í° ë¹„ìš©ì´ ë“­ë‹ˆë‹¤(í—ˆìœ„ ìŒìˆ˜ê°€ ë§ì´ ì¡´ì¬í•¨). ì˜¤ë¥˜ê°€ ì¡´ì¬í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  ì´ ê²½ìš° íŒŒì‚°ì— ê°€ê¹Œìš´ ê´€ì°°ì´ ì•„ë‹Œ ê²ƒì„ ê·¸ ë°˜ëŒ€ì˜ ê²½ìš°ë³´ë‹¤ íŒŒì‚°ì— ê°€ê¹Œìš´ ê²ƒìœ¼ë¡œ ì‹ë³„í•˜ëŠ” ê²ƒì´ ë” ë‚«ë‹¤ê³  ìƒê°í•˜ë¯€ë¡œ ìœ ìš©í•œ ëª¨ë¸ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.","metadata":{}},{"cell_type":"code","source":"# Testing\n\ntest_pred_lr = best_est_reg.predict(X_test)\n#test_pred_rf = best_est_rfc.predict(X_test)\n#test_pred_xgb = best_est_xgb.predict(X_test) \ntest_pred_cat = best_est_cat.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting confusion matrix for each classifier\n\nconf_mx0 = confusion_matrix(y_test,test_pred_lr)\nconf_mx1 = confusion_matrix(y_test,test_pred_cat)\n\nheat_cm0 = pd.DataFrame(conf_mx0, columns=np.unique(y_test), index = np.unique(y_test))\nheat_cm0.index.name = 'Actual'\nheat_cm0.columns.name = 'Predicted'\n\nheat_cm1 = pd.DataFrame(conf_mx1, columns=np.unique(y_test), index = np.unique(y_test))\nheat_cm1.index.name = 'Actual'\nheat_cm1.columns.name = 'Predicted'\n\nf, ax = plt.subplots(1, 2, figsize=(15,8))\nf.subplots_adjust(left=None, bottom=None, right= 2, top=None, wspace=None, hspace= None)\n\nsns.heatmap(heat_cm0, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[0])\nax[0].set_title('Logistic Regression', fontsize = 20)\nsns.heatmap(heat_cm1, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[1])\nax[1].set_title('Catboot Classifier', fontsize = 20)\n\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, test_pred_lr, target_names=label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, test_pred_cat, target_names=label))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"ê²€ì¦ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œì‹œëœ ê²ƒê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ê²Œ ìº£ë¶€ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ë©´ ê³ ë ¤ëœ ì§€í‘œ(F1)ê°€ ì–´ë–»ê²Œ ë” ë†’ì€ì§€ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ì´ ê²½ìš° ì†Œìˆ˜ì ê³„ì¸µì´ íŒŒì‚°ì— ê·¼ì ‘í•˜ì§€ ì•Šì€ ì¼ë¶€ ê¸°ì—…ì„ íŒŒì‚°ì— ê·¼ì ‘í•œ ê²ƒìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜í•˜ëŠ” ê²ƒì„ ë” ì˜ ì¸ì‹í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìµœì„ ì˜ ê²°ì •ì…ë‹ˆë‹¤.\n\nì§€ì›í•´ ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦¬ë©° ë‹¤ìŒ ë…¸íŠ¸ë¶ìœ¼ë¡œ ëµ™ê² ìŠµë‹ˆë‹¤!","metadata":{}}]}