{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1938459,
          "sourceType": "datasetVersion",
          "datasetId": 1111894
        }
      ],
      "dockerImageVersionId": 30055,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "Bankruptcy_DetectionğŸ’°ğŸ“ˆ",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "\n",
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n",
        "# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from tempfile import NamedTemporaryFile\n",
        "from urllib.request import urlopen\n",
        "from urllib.parse import unquote, urlparse\n",
        "from urllib.error import HTTPError\n",
        "from zipfile import ZipFile\n",
        "import tarfile\n",
        "import shutil\n",
        "\n",
        "CHUNK_SIZE = 40960\n",
        "DATA_SOURCE_MAPPING = 'company-bankruptcy-prediction:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F1111894%2F1938459%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240930%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240930T023858Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D4096b4499576305a91e6c697ecbe71463de7f77ae70e4bf0c7d1f284da6da5e6a14d6f22fc8a62beb15d175eb0f8f32d42e9261c8843120542356751ed33fa5d2fbef8b6b2866029cf740de96a6114612ab0cf8ba0248197ba93b30d0c43ff755d149533d3229b986d4f5e18cccacee12d8e7a74c339606d800777b48a5d1c022085fd6a4c9a55089941e827b1aeb94f11ecc1f864ee878c0b5a59dfb7c2ee7cd3c28617b1e3cfb5187d3679b0d10a2f6602636ccd4cdf77bd0b40f466baaef86e0409472a7c3fe904c821ff3054cca99ca6df0d77502f3c2712bb75dfa3e74870ec72df51750886842a4ed2211cb435cb00a956d180b65f582bba01b39f0825'\n",
        "\n",
        "KAGGLE_INPUT_PATH='/kaggle/input'\n",
        "KAGGLE_WORKING_PATH='/kaggle/working'\n",
        "KAGGLE_SYMLINK='kaggle'\n",
        "\n",
        "!umount /kaggle/input/ 2> /dev/null\n",
        "shutil.rmtree('/kaggle/input', ignore_errors=True)\n",
        "os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n",
        "os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n",
        "\n",
        "try:\n",
        "  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "try:\n",
        "  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n",
        "except FileExistsError:\n",
        "  pass\n",
        "\n",
        "for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n",
        "    directory, download_url_encoded = data_source_mapping.split(':')\n",
        "    download_url = unquote(download_url_encoded)\n",
        "    filename = urlparse(download_url).path\n",
        "    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n",
        "    try:\n",
        "        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n",
        "            total_length = fileres.headers['content-length']\n",
        "            print(f'Downloading {directory}, {total_length} bytes compressed')\n",
        "            dl = 0\n",
        "            data = fileres.read(CHUNK_SIZE)\n",
        "            while len(data) > 0:\n",
        "                dl += len(data)\n",
        "                tfile.write(data)\n",
        "                done = int(50 * dl / int(total_length))\n",
        "                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n",
        "                sys.stdout.flush()\n",
        "                data = fileres.read(CHUNK_SIZE)\n",
        "            if filename.endswith('.zip'):\n",
        "              with ZipFile(tfile) as zfile:\n",
        "                zfile.extractall(destination_path)\n",
        "            else:\n",
        "              with tarfile.open(tfile.name) as tarfile:\n",
        "                tarfile.extractall(destination_path)\n",
        "            print(f'\\nDownloaded and uncompressed: {directory}')\n",
        "    except HTTPError as e:\n",
        "        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n",
        "        continue\n",
        "    except OSError as e:\n",
        "        print(f'Failed to load {download_url} to path {destination_path}')\n",
        "        continue\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "a7sPGVfVy6ry"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## íšŒì‚¬ íŒŒì‚° ğŸ“‰ğŸ’¸\n",
        "\n",
        "\n",
        "ëŒ€ë§Œ ì¦ê¶Œê±°ë˜ì†Œì˜ ë¹„ì¦ˆë‹ˆìŠ¤ ê·œì •ì— ë”°ë¼ íšŒì‚¬ íŒŒì‚°ì„ ë‚˜íƒ€ë‚´ëŠ” 1999~2009ë…„ ëŒ€ë§Œ ê²½ì œ ì €ë„ì˜ ë°ì´í„°ë‹¤.\n",
        "\n",
        "**ì†Œê°œ**\n",
        "\n",
        "ì´ ì»¤ë„ì—ì„œëŠ” ë‹¤ì–‘í•œ ì˜ˆì¸¡ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í–¥í›„ íŒŒì‚°ì— ì§ë©´í•  ê¸°ì—…ì„ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•  ìˆ˜ ìˆëŠ”ì§€ ì—¬ë¶€ë¥¼ ê°ì§€í•˜ëŠ” ë° ì–¼ë§ˆë‚˜ ì •í™•í•œì§€ í™•ì¸í•  ê²ƒì…ë‹ˆë‹¤. ë°ì´í„° ì„¹ì…˜ì— ì„¤ëª…ëœ ëŒ€ë¡œ ë°ì´í„° ì„¸íŠ¸ì—ëŠ” ë‹¤ìŒì´ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:\n",
        "\n",
        "- 95ê°œ ê¸°ëŠ¥(X1-X95, ëŒ€ë§Œ ì¦ê¶Œê±°ë˜ì†Œ ì—…ë¬´ ê·œì •)\n",
        "- 1 ë¼ë²¨ ë²¡í„°\n",
        "\n",
        "ì´ í”„ë¡œì íŠ¸ì˜ ëª©í‘œëŠ” ì´ëŸ¬í•œ ê¸°ëŠ¥ì„ ì‚¬ìš©í•˜ì—¬ ì„ íƒëœ ëª¨ë¸ì— ë¯¸ì¹˜ëŠ” ì˜í–¥/ì—­í• ì„ íŒŒì•…í•˜ê³  ë±…í¬ëŸ½ì— ê°€ê¹Œìš´ ê¸°ì—…ì„ ì¸ì‹í•˜ëŠ” ë° ì–´ë–»ê²Œ ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ”ì§€ íŒŒì•…í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì‹œì‘í•©ë‹ˆë‹¤!\n",
        "\n",
        "ì¶”ì‹ : ì´ ë…¸íŠ¸ë¶ì´ ë§ˆìŒì— ë“œì‹ ë‹¤ë©´ **UPVOTE**ë¥¼ ìŠì§€ ë§ˆì„¸ìš”!\n",
        "\n"
      ],
      "metadata": {
        "id": "OZtBqM8qy6r1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTING LIBRARIES\n",
        "\n",
        "# General Libraries\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from scipy.stats import norm\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action=\"ignore\")\n",
        "\n",
        "# Preprocessing Libraries\n",
        "\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "# Machine Learning Libraries\n",
        "\n",
        "import sklearn\n",
        "import xgboost as xgb\n",
        "from sklearn import tree\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_curve\n",
        "from imblearn.pipeline import Pipeline\n",
        "from catboost import CatBoostClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from mlxtend.classifier import StackingCVClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import KFold, cross_validate\n",
        "from sklearn.metrics import recall_score, f1_score, roc_auc_score\n",
        "from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\n",
        "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold, StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score\n",
        "\n",
        "\n",
        "\n",
        "# Defining the working directory\n",
        "\n",
        "input_path = '../input/company-bankruptcy-prediction/'"
      ],
      "metadata": {
        "trusted": true,
        "id": "wYCRDduYy6r5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# IMPORTING DATA\n",
        "\n",
        "bank_data = pd.read_csv(input_path + 'data.csv')\n",
        "bank_data.head()"
      ],
      "metadata": {
        "trusted": true,
        "id": "5GZkLC6Jy6r6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bank_data.info()"
      ],
      "metadata": {
        "trusted": true,
        "id": "dBMGCJEly6r8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ì œ ë°ì´í„°ì— ëŒ€í•œ ì•„ì´ë””ì–´ê°€ ìƒê²¼ìœ¼ë‹ˆ ë°ì´í„°ì— ëŒ€í•œ ë” ë§ì€ ì •ë³´ë¥¼ ì–»ì–´ì•¼ í•©ë‹ˆë‹¤. ê°€ì¥ ë¨¼ì € ì´í•´í•˜ê³  ì‹¶ì€ ê²ƒì€ ë°ì´í„°ì˜ íŠ¹ì„±, ì¦‰ ë°ì´í„°ê°€ ìˆ˜ì¹˜ ë˜ëŠ” ë²”ì£¼í˜•ì¸ì§€, ê·¸ë¦¬ê³  ê·¸ ì¤‘ì— ëˆ„ë½ëœ ì •ë³´ê°€ ìˆëŠ”ì§€ ì—¬ë¶€ì…ë‹ˆë‹¤. info() ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ ë‘ ê°€ì§€ ì ì„ ëª¨ë‘ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "ê²°ê³¼ íŒ¨ë„ì€ ìš°ë¦¬ì—ê²Œ ê°•ë ¥í•œ ì •ë³´ë¥¼ ì œê³µí•˜ë©°, ê·¸ ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤:\n",
        "\n",
        "ë°ì´í„° ì„¸íŠ¸ëŠ” 96ê°œì˜ features ê°ê°ì— ëŒ€í•´ 6819ê°œì˜ ê´€ì°° ê²°ê³¼ ì¡°í•©ìœ¼ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤.\n",
        "ëª¨ë“  featuresì€ ìˆ«ì(int64 ë˜ëŠ” float64)ì…ë‹ˆë‹¤\n",
        "ë°ì´í„° ì¤‘ ëˆ„ë½ëœ ê°’(Nan)ì´ ì—†ìŠµë‹ˆë‹¤\n",
        "ëª¨ë“  featuresê°€ ìˆ«ìì´ê¸° ë•Œë¬¸ì— ì¶”ê°€ ì •ë³´ ì¶œì²˜ì¸ ê¸°ìˆ  í†µê³„ë¥¼ ì‰½ê²Œ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "E4jc3Cl3y6r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the descriptive statistics of our numrerical features\n",
        "\n",
        "bank_data.describe()"
      ],
      "metadata": {
        "trusted": true,
        "id": "SG2eTm5Cy6r_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ëˆ„ë½ëœ ê°’ì´ ì—†ë‹¤ëŠ” ê²ƒì„ ì´ë¯¸ ì•Œê³  ìˆì§€ë§Œ(ì—¬ê¸°ì„œëŠ” 96ê°œì˜ featureë§Œ ìˆë‹¤ëŠ” ì ì„ ê³ ë ¤í•˜ë©´ ë§¤ìš° ì‰½ìŠµë‹ˆë‹¤), í”„ë¡œì íŠ¸ì˜ ë‹¤ìŒ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ì™€ ì‹œê°„ ë‚­ë¹„ë¥¼ ë°©ì§€í•˜ë ¤ë©´ ì´ê²ƒì´ ì‚¬ì‹¤ì¸ì§€ ê³„ì‚°ì ìœ¼ë¡œ í™•ì¸í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "1BPogP46y6sA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking Nan presence\n",
        "\n",
        "#bank_data.isna().sum().max()\n",
        "[print(col) for col in bank_data if bank_data[col].isna().sum() > 0]"
      ],
      "metadata": {
        "trusted": true,
        "id": "JsfnOYjUy6sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë°ì´í„°ì— ì¤‘ë³µì´ ìˆì„ ê°€ëŠ¥ì„±ì— ëŒ€í•´ ì¶”ê°€ë¡œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ì¤‘ë³µì€ ë°ì´í„°ì— ì¤‘ë³µì„ ì¼ìœ¼í‚¬ ìˆ˜ ìˆëŠ” ë™ì¼í•œ ê´€ì°°ì´ë¯€ë¡œ ì‚­ì œí•´ì•¼ í•©ë‹ˆë‹¤. ì œê±°í•  ì¤‘ë³µì´ ìˆëŠ”ì§€ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤:"
      ],
      "metadata": {
        "id": "NOqQSj7sy6sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking for duplicates\n",
        "\n",
        "bank_data.duplicated().sum()"
      ],
      "metadata": {
        "trusted": true,
        "id": "CjXYSC5Wy6sB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ë°ì´í„°ì— ì¤‘ë³µì´ ìˆì„ ê°€ëŠ¥ì„±ì— ëŒ€í•´ ì¶”ê°€ë¡œ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ì¤‘ë³µì€ ë°ì´í„°ì— ì¤‘ë³µì„ ì¼ìœ¼í‚¬ ìˆ˜ ìˆëŠ” ë™ì¼í•œ ê´€ì°°ì´ë¯€ë¡œ ì‚­ì œí•´ì•¼ í•©ë‹ˆë‹¤. ì œê±°í•  ì¤‘ë³µì´ ìˆëŠ”ì§€ í™•ì¸í•´ ë³´ê² ìŠµë‹ˆë‹¤:"
      ],
      "metadata": {
        "id": "ylbjOszMy6sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The classes are heavily skewed we need to solve this issue later.\n",
        "\n",
        "print(bank_data['Bankrupt?'].value_counts())\n",
        "print('-'* 30)\n",
        "print('Financially stable: ', round(bank_data['Bankrupt?'].value_counts()[0]/len(bank_data) * 100,2), '% of the dataset')\n",
        "print('Financially unstable: ', round(bank_data['Bankrupt?'].value_counts()[1]/len(bank_data) * 100,2), '% of the dataset')"
      ],
      "metadata": {
        "trusted": true,
        "id": "PqVdrcqUy6sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking labels distributions\n",
        "\n",
        "sns.set_theme(context = 'paper')\n",
        "\n",
        "plt.figure(figsize = (10,5))\n",
        "sns.countplot(bank_data['Bankrupt?'])\n",
        "plt.title('Class Distributions \\n (0: Fin. Stable || 1: Fin. Unstable)', fontsize=14)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "6JeTnCvmy6sC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ìœ„ì˜ í”Œë¡¯ì„ ë³´ë©´ ë ˆì´ë¸”ì˜ ë¶ˆê· í˜•ì´ ì–¼ë§ˆë‚˜ ì‹¬í•œì§€ ëª…í™•í•˜ê²Œ ì•Œ ìˆ˜ ìˆìœ¼ë©°, ì´ëŠ” ì¢‹ì€ ì„±ëŠ¥ì„ ì–»ê¸° ìœ„í•´ í•´ê²°í•´ì•¼ í•  ì£¼ìš” ì¥ì• ë¬¼ì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "zblOjFbQy6sD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exploratory Data Analysis (EDA).\n",
        "\n",
        "ì—¬ëŸ¬ ìœ í˜•ì˜ ì‹œê°í™”ì—ì„œ ê°€ëŠ¥í•œ í•œ ë§ì€ ì •ë³´ë¥¼ ì–»ê¸° ìœ„í•´ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚´í´ë´…ì‹œë‹¤. ê°€ì¥ ë¨¼ì € ë³´ì—¬ì¤„ ì¼ë°˜ì ì¸ í”Œë¡¯ì€ ë°ì´í„° ì„¸íŠ¸ì˜ ìˆ˜ì¹˜ ë³€ìˆ˜ ê°„ì˜ ëª¨ë“  ìƒê´€ê´€ê³„(ì„ íƒí•œ í…ŒìŠ¤íŠ¸ì— ë”°ë¼ ì„ í˜• ë° ë¹„ ì„ í˜•)ë¥¼ í”Œë¡¯í•˜ëŠ” ìƒê´€ê´€ê³„ íˆíŠ¸ë§µì…ë‹ˆë‹¤: í”¼ì–´ìŠ¨ ë˜ëŠ” ìŠ¤í”¼ì–´ë§¨. ê¸°ëŠ¥ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ íŒŒì•…í•˜ëŠ” ê²ƒì€ í›ˆë ¨ ì¤‘ì— ìœ ì§€í•´ì•¼ í•  ê¸°ëŠ¥ì„ ê²°ì •í•˜ëŠ” ë° ì¤‘ìš”í•œ ë‹¨ê³„ì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "ggKyDTSAy6sD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at the histograms of numerical data\n",
        "\n",
        "bank_data.hist(figsize = (35,30), bins = 50)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "6ZNelWXCy6sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# EDA & VISUALIZATIONS\n",
        "\n",
        "# Correlation Heatmap (Spearman)\n",
        "\n",
        "f, ax = plt.subplots(figsize=(30, 25))\n",
        "mat = bank_data.corr('spearman')\n",
        "mask = np.triu(np.ones_like(mat, dtype=bool))\n",
        "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
        "sns.heatmap(mat, mask=mask, cmap=cmap, vmax=1, center=0,# annot = True,\n",
        "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "VRlb3pgay6sD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Boxplots of the numerical features\n",
        "\n",
        "plt.figure(figsize = (20,20))\n",
        "ax =sns.boxplot(data = bank_data, orient=\"h\")\n",
        "ax.set_title('Bank Data Boxplots', fontsize = 18)\n",
        "ax.set(xscale=\"log\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Xa6PoIYHy6sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting interesting features\n",
        "\n",
        "f, axes = plt.subplots(ncols=4, figsize=(24,6))\n",
        "\n",
        "sns.boxplot(x='Bankrupt?', y=\" Net Income to Total Assets\", data=bank_data, ax=axes[0])\n",
        "axes[0].set_title('Bankrupt vs Net Income to Total Assets')\n",
        "\n",
        "sns.boxplot(x='Bankrupt?', y=\" Total debt/Total net worth\", data=bank_data, ax=axes[1])\n",
        "axes[1].set_title('Bankrupt vs Tot Debt/Net worth Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x='Bankrupt?', y=\" Debt ratio %\", data=bank_data, ax=axes[2])\n",
        "axes[2].set_title('Bankrupt vs Debt ratio Correlation')\n",
        "\n",
        "\n",
        "sns.boxplot(x='Bankrupt?', y=\" Net worth/Assets\", data=bank_data, ax=axes[3])\n",
        "axes[3].set_title('Bankrupt vs Net Worth/Assets Correlation')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "Dlg44Cq4y6sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting other interesting features\n",
        "\n",
        "f, axes = plt.subplots(ncols=4, figsize=(24,6))\n",
        "\n",
        "sns.boxplot(x='Bankrupt?', y=\" Working Capital to Total Assets\", data=bank_data, ax=axes[0])\n",
        "axes[0].set_title('Bankrupt vs  working capital to total assets')\n",
        "\n",
        "sns.boxplot(x='Bankrupt?', y=\" Cash/Total Assets\", data=bank_data, ax=axes[1])\n",
        "axes[1].set_title('Bankrupt vs cash / total assets')\n",
        "\n",
        "\n",
        "sns.boxplot(x='Bankrupt?', y=\" Current Liability to Assets\", data=bank_data, ax=axes[2])\n",
        "axes[2].set_title('Bankrupt vs current liability to assets')\n",
        "\n",
        "\n",
        "sns.boxplot(x='Bankrupt?', y=\" Retained Earnings to Total Assets\", data=bank_data, ax=axes[3])\n",
        "axes[3].set_title('Bankrupt vs  Retained Earnings to Total Assets')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "1kljPeDqy6sE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ì œ íŒŒì‚°ì´ ì„ë°•í•œ ê¸°ì—…ì— ëŒ€í•œ ì´ëŸ¬í•œ ê¸°ëŠ¥ì˜ ë¶„í¬ë¥¼ ì‚´í´ë´…ì‹œë‹¤:"
      ],
      "metadata": {
        "id": "fD2EKUj7y6sE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the feature distributions for close to bankrputcy companies\n",
        "\n",
        "f, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n",
        "\n",
        "cash_flow_rate = bank_data[' Net Income to Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\n",
        "sns.distplot(cash_flow_rate,ax=ax1, fit=norm, color='#FB8861')\n",
        "ax1.set_title(' Net Income to Total Assets \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "tot_debt_net = bank_data[' Total debt/Total net worth'].loc[bank_data['Bankrupt?'] == 1].values\n",
        "sns.distplot(tot_debt_net ,ax=ax2, fit=norm, color='#56F9BB')\n",
        "ax2.set_title('total debt/tot net worth \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "\n",
        "debt_ratio = bank_data[' Debt ratio %'].loc[bank_data['Bankrupt?'] == 1].values\n",
        "sns.distplot(debt_ratio,ax=ax3, fit=norm, color='#C5B3F9')\n",
        "ax3.set_title('debt_ratio \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "net_worth_assets = bank_data[' Net worth/Assets'].loc[bank_data['Bankrupt?'] == 1].values\n",
        "sns.distplot(net_worth_assets,ax=ax4, fit=norm, color='#C5B3F9')\n",
        "ax4.set_title('net worth/assets \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "5et6T0kDy6sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n",
        "\n",
        "working_cap = bank_data[' Working Capital to Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\n",
        "sns.distplot(working_cap,ax=ax1, fit=norm, color='#FB8861')\n",
        "ax1.set_title('working capitals to total assets \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "cash_tot_assets = bank_data[' Cash/Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\n",
        "sns.distplot(cash_tot_assets ,ax=ax2, fit=norm, color='#56F9BB')\n",
        "ax2.set_title('cash/total assets \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "\n",
        "asset_liab = bank_data[' Current Liability to Assets'].loc[bank_data['Bankrupt?'] == 1].values\n",
        "sns.distplot(asset_liab,ax=ax3, fit=norm, color='#C5B3F9')\n",
        "ax3.set_title('liability to assets \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "operating_funds = bank_data[' Retained Earnings to Total Assets'].loc[bank_data['Bankrupt?'] == 1].values\n",
        "sns.distplot(operating_funds,ax=ax4, fit=norm, color='#C5B3F9')\n",
        "ax4.set_title('retain earnings to total assets \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "zADmhJ0fy6sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ìƒê°’ ì œê±°\n",
        "\n",
        "ì´ í”„ë¡œì íŠ¸ì—ì„œëŠ” ê°€ì¥ ê·¹ë‹¨ì ì¸ ì´ìƒê°’ì„ ì œê±°í•˜ë ¤ê³  í•©ë‹ˆë‹¤(ì´ë¥¼ ì œê±°í•˜ëŠ” ëŒ€ì‹  í‰ê·  ë˜ëŠ” ì¤‘ì•™ê°’ìœ¼ë¡œ ì¶”ì •í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤). ì´ë ‡ê²Œ í•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì´ í–¥ìƒë  ê²ƒì…ë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "NoJf1vK4y6sF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Outliers removal\n",
        "\n",
        "def outliers_removal(feature,feature_name,dataset):\n",
        "\n",
        "    # Identify 25th & 75th quartiles\n",
        "\n",
        "    q25, q75 = np.percentile(feature, 25), np.percentile(feature, 75)\n",
        "    print('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n",
        "    feat_iqr = q75 - q25\n",
        "    print('iqr: {}'.format(feat_iqr))\n",
        "\n",
        "    feat_cut_off = feat_iqr * 1.5\n",
        "    feat_lower, feat_upper = q25 - feat_cut_off, q75 + feat_cut_off\n",
        "    print('Cut Off: {}'.format(feat_cut_off))\n",
        "    print(feature_name +' Lower: {}'.format(feat_lower))\n",
        "    print(feature_name +' Upper: {}'.format(feat_upper))\n",
        "\n",
        "    outliers = [x for x in feature if x < feat_lower or x > feat_upper]\n",
        "    print(feature_name + ' outliers for close to bankruptcy cases: {}'.format(len(outliers)))\n",
        "    #print(feature_name + ' outliers:{}'.format(outliers))\n",
        "\n",
        "    dataset = dataset.drop(dataset[(dataset[feature_name] > feat_upper) | (dataset[feature_name] < feat_lower)].index)\n",
        "    print('-' * 65)\n",
        "\n",
        "    return dataset\n",
        "\n",
        "for col in bank_data:\n",
        "    new_df = outliers_removal(bank_data[col],str(col),bank_data)\n",
        ""
      ],
      "metadata": {
        "trusted": true,
        "id": "r0N-Xi13y6sF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ì´ì œ ì²­ì†Œëœ ìƒì ê·¸ë¦¼ì„ ì‚´í´ë´…ì‹œë‹¤:"
      ],
      "metadata": {
        "id": "d-CVdunjy6sG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f,(ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24,6))\n",
        "\n",
        "# Boxplots with outliers removed\n",
        "\n",
        "sns.boxplot(x=\"Bankrupt?\", y=\" Net Income to Total Assets\", data=new_df,ax=ax1)\n",
        "ax1.set_title(\"Net Income to Total Assets \\n Reduction of outliers\", fontsize=14)\n",
        "\n",
        "sns.boxplot(x=\"Bankrupt?\", y=\" Total debt/Total net worth\", data=new_df,ax=ax2)\n",
        "ax2.set_title(\"total debt/total net worth \\n Reduction of outliers\", fontsize=14)\n",
        "\n",
        "sns.boxplot(x=\"Bankrupt?\", y=\" Debt ratio %\", data=new_df,ax=ax3)\n",
        "ax3.set_title(\"debt ratio % \\n Reduction of outliers\", fontsize=14)\n",
        "\n",
        "sns.boxplot(x=\"Bankrupt?\", y=' Net worth/Assets', data=new_df,ax=ax4)\n",
        "ax4.set_title(\"net worth/assets \\n Reduction of outliers\", fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "EOeYrfZQy6sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f,(ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(24,6))\n",
        "\n",
        "# Boxplots with outliers removed\n",
        "\n",
        "sns.boxplot(x=\"Bankrupt?\", y=' Working Capital to Total Assets', data=new_df,ax=ax1)\n",
        "ax1.set_title(\"working capital to total assets \\n Reduction of outliers\", fontsize=14)\n",
        "\n",
        "sns.boxplot(x=\"Bankrupt?\", y=' Cash/Total Assets', data=new_df,ax=ax2)\n",
        "ax2.set_title(\"cash / total assets \\n Reduction of outliers\", fontsize=14)\n",
        "\n",
        "sns.boxplot(x=\"Bankrupt?\", y=' Current Liability to Assets', data=new_df,ax=ax3)\n",
        "ax3.set_title(\"current liability to assets \\n Reduction of outliers\", fontsize=14)\n",
        "\n",
        "sns.boxplot(x=\"Bankrupt?\", y=' Retained Earnings to Total Assets', data=new_df,ax=ax4)\n",
        "ax4.set_title(\"Retained Earnings to Total Assets \\n Reduction of outliers\", fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "z3qcn1DCy6sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the feature distributions for close to bankrputcy companies\n",
        "\n",
        "f, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n",
        "\n",
        "cash_flow_rate = new_df[' Net Income to Total Assets'].loc[new_df['Bankrupt?'] == 1].values\n",
        "sns.distplot(cash_flow_rate,ax=ax1, fit=norm, color='#FB8861')\n",
        "ax1.set_title(' Net Income to Total Assets \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "tot_debt_net = new_df[' Total debt/Total net worth'].loc[new_df['Bankrupt?'] == 1].values\n",
        "sns.distplot(tot_debt_net ,ax=ax2, fit=norm, color='#56F9BB')\n",
        "ax2.set_title('total debt/tot net worth \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "\n",
        "debt_ratio = new_df[' Debt ratio %'].loc[new_df['Bankrupt?'] == 1].values\n",
        "sns.distplot(debt_ratio,ax=ax3, fit=norm, color='#C5B3F9')\n",
        "ax3.set_title('debt_ratio \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "net_worth_assets = new_df[' Net worth/Assets'].loc[new_df['Bankrupt?'] == 1].values\n",
        "sns.distplot(net_worth_assets,ax=ax4, fit=norm, color='#C5B3F9')\n",
        "ax4.set_title('net worth/assets \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "pdHguxHBy6sG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "f, (ax1, ax2, ax3, ax4) = plt.subplots(1,4, figsize=(24, 6))\n",
        "\n",
        "working_cap = new_df[' Working Capital to Total Assets'].loc[new_df['Bankrupt?'] == 1].values\n",
        "sns.distplot(working_cap,ax=ax1, fit=norm, color='#FB8861')\n",
        "ax1.set_title('working capitals to total assets \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "cash_tot_assets = new_df[' Cash/Total Assets'].loc[new_df['Bankrupt?'] == 1].values\n",
        "sns.distplot(cash_tot_assets ,ax=ax2, fit=norm, color='#56F9BB')\n",
        "ax2.set_title('cash/total assets \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "\n",
        "asset_liab = new_df[' Current Liability to Assets'].loc[new_df['Bankrupt?'] == 1].values\n",
        "sns.distplot(asset_liab,ax=ax3, fit=norm, color='#C5B3F9')\n",
        "ax3.set_title('liability to assets \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "operating_funds = new_df[' Retained Earnings to Total Assets'].loc[new_df['Bankrupt?'] == 1].values\n",
        "sns.distplot(operating_funds,ax=ax4, fit=norm, color='#C5B3F9')\n",
        "ax4.set_title('retain earnings to total assets \\n (Unstable companies)', fontsize=14)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "_kg_hide-input": true,
        "trusted": true,
        "id": "RJfiSU6Ty6sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ìš°ë¦¬ê°€ ë³¼ ìˆ˜ ìˆëŠ” ê²ƒì€ ê·¹ë‹¨ì ì¸ ì´ìƒê°’ì„ ì œê±°í•˜ë©´ ë” ë§ì€ \"ì¢… ëª¨ì–‘\" ë¶„í¬ë¥¼ ì–»ëŠ” ë° í™•ì‹¤íˆ ë„ì›€ì´ ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤! (ì ì–´ë„ í‘œì‹œëœ featuresì˜ ê²½ìš°)"
      ],
      "metadata": {
        "id": "9-EdYiYey6sH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividing Data and Labels\n",
        "\n",
        "labels = new_df['Bankrupt?']\n",
        "new_df = new_df.drop(['Bankrupt?'], axis = 1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "UsAxEEeFy6sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def log_trans(data):\n",
        "\n",
        "    for col in data:\n",
        "        skew = data[col].skew()\n",
        "        if skew > 0.5 or skew < -0.5:\n",
        "            data[col] = np.log1p(data[col])\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    return data\n",
        "\n",
        "data_norm = log_trans(new_df)"
      ],
      "metadata": {
        "trusted": true,
        "id": "oKjJd4Moy6sH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Boxplots of the preprocessed numerical features\n",
        "\n",
        "plt.figure(figsize = (20,20))\n",
        "ax =sns.boxplot(data = data_norm, orient=\"h\")\n",
        "ax.set_title('Bank Data Preprocessed Boxplots', fontsize = 18)\n",
        "ax.set(xscale=\"log\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "avK2MWMjy6sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_norm.hist(figsize = (35,30),bins = 50)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "rPVtiPq1y6sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting Train and Test Data\n",
        "\n",
        "X_raw,X_test,y_raw,y_test  = train_test_split(data_norm,\n",
        "                                              labels,\n",
        "                                              test_size=0.1,\n",
        "                                              stratify = labels,\n",
        "                                              random_state = 42)"
      ],
      "metadata": {
        "trusted": true,
        "id": "ClQUAkPAy6sI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE\n",
        "ì˜¤ë²„ìƒ˜í”Œë§ ê¸°ë²•ìœ¼ë¡œ SMOTEë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ë¬´ì‘ìœ„ ì–¸ë”ìƒ˜í”Œë§ë³´ë‹¤ SMOTEê°€ ë” ì •í™•í•  ê°€ëŠ¥ì„±ì´ ë†’ì§€ë§Œ, ì•ì„œ ì–¸ê¸‰í•œ ê²ƒì²˜ëŸ¼ í–‰ì´ ì œê±°ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— í›ˆë ¨í•˜ëŠ” ë° ë” ë§ì€ ì‹œê°„ì´ ê±¸ë¦´ ê²ƒì…ë‹ˆë‹¤.\n",
        "\n",
        "SMOTEì•Œê³ ë¦¬ì¦˜ì€ ì˜¤ë²„ìƒ˜í”Œë§ ê¸°ë²• ì¤‘ í•©ì„±ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ìœ¼ë¡œ ê°€ì¥ ë§ì´ ì‚¬ìš©ë˜ê³  ìˆëŠ” ëª¨ë¸ì´ë‹¤\n",
        "SMOTE(synthetic minority oversampling technique)ë€, í•©ì„± ì†Œìˆ˜ ìƒ˜í”Œë§ ê¸°ìˆ ë¡œ ë‹¤ìˆ˜ í´ë˜ìŠ¤ë¥¼ ìƒ˜í”Œë§í•˜ê³  ê¸°ì¡´ ì†Œìˆ˜ ìƒ˜í”Œì„ ë³´ê°„í•˜ì—¬ ìƒˆë¡œìš´ ì†Œìˆ˜ ì¸ìŠ¤í„´ìŠ¤ë¥¼ í•©ì„±í•´ë‚¸ë‹¤.\n",
        "ì¼ë°˜ì ì¸ ê²½ìš° ì„±ê³µì ìœ¼ë¡œ ì‘ë™í•˜ì§€ë§Œ, ì†Œìˆ˜ë°ì´í„°ë“¤ ì‚¬ì´ë¥¼ ë³´ê°„í•˜ì—¬ ì‘ë™í•˜ê¸° ë•Œë¬¸ì— ëª¨ë¸ë§ì…‹ì˜ ì†Œìˆ˜ë°ì´í„°ë“¤ ì‚¬ì´ì˜ íŠ¹ì„±ë§Œì„ ë°˜ì˜í•˜ê³  ìƒˆë¡œìš´ ì‚¬ë¡€ì˜ ë°ì´í„° ì˜ˆì¸¡ì—” ì·¨ì•½í•  ìˆ˜ ìˆë‹¤.\n",
        "\n",
        "SMOTE ë™ì‘ ë°©ì‹\n",
        "ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ì´ë‚˜ KNN(ìµœê·¼ì ‘ì´ì›ƒ) ëª¨ë¸ ê¸°ë²•ì„ í™œìš©í•œë‹¤.\n",
        "\n",
        "ì†Œìˆ˜ ë°ì´í„° ì¤‘ íŠ¹ì • ë²¡í„° (ìƒ˜í”Œ)ì™€ ê°€ì¥ ê°€ê¹Œìš´ ì´ì›ƒ ì‚¬ì´ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•œë‹¤.\n",
        "ì´ ì°¨ì´ì— 0ê³¼ 1ì‚¬ì´ì˜ ë‚œìˆ˜ë¥¼ ê³±í•œë‹¤.\n",
        "íƒ€ê²Ÿ ë²¡í„°ì— ì¶”ê°€í•œë‹¤.\n",
        "ë‘ ê°œì˜ íŠ¹ì • ê¸°ëŠ¥ ì‚¬ì´ì˜ ì„ ë¶„ì„ ë”°ë¼ ì„ì˜ì˜ ì ì„ ì„ íƒí•  ìˆ˜ ìˆë‹¤."
      ],
      "metadata": {
        "id": "LpXGULdiy6sI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MODELING\n",
        "\n",
        "\n",
        "ì´ì œ ëª¨ë¸ë¡œ ë¬´ì—‡ì„ í•  ìˆ˜ ìˆëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤! ì •ë¦¬ëœ ì–¸ë”ìƒ˜í”Œë§ ë°ì´í„°ì™€ SMOTE ì˜¤ë²„ìƒ˜í”Œë§ ë°ì´í„°ì˜ ì„±ëŠ¥ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë¶€ë¶„ì„ ìœ„í•´ ì €ëŠ” ëª‡ ê°€ì§€ ë‹¤ë¥¸ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸°ë¡œ ê²°ì •í–ˆìŠµë‹ˆë‹¤:\n",
        "\n",
        "Logistic Regression\n",
        "SVC\n",
        "Random Forest Classifier\n",
        "CatBoost Classifier"
      ],
      "metadata": {
        "id": "nMJbdJUVy6sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stratified Cross Validation Splitting\n",
        "\n",
        "sss = StratifiedKFold(n_splits=5, random_state=42, shuffle=False)\n",
        "\n",
        "for train_index, test_index in sss.split(X_raw,y_raw):\n",
        "\n",
        "    print(\"Train:\", train_index, \"Test:\", test_index)\n",
        "    X_train_sm, X_val_sm = X_raw.iloc[train_index], X_raw.iloc[test_index]\n",
        "    y_train_sm, y_val_sm = y_raw.iloc[train_index], y_raw.iloc[test_index]\n",
        "\n",
        "# Check the Distribution of the labels\n",
        "\n",
        "\n",
        "# Turn into an array\n",
        "X_train_sm = X_train_sm.values\n",
        "X_val_sm = X_val_sm.values\n",
        "y_train_sm = y_train_sm.values\n",
        "y_val_sm = y_val_sm.values\n",
        "\n",
        "# See if both the train and test label distribution are similarly distributed\n",
        "train_unique_label, train_counts_label = np.unique(y_train_sm, return_counts=True)\n",
        "test_unique_label, test_counts_label = np.unique(y_val_sm, return_counts=True)\n",
        "print('-' * 84)\n",
        "\n",
        "print('Label Distributions: \\n')\n",
        "print(train_counts_label/ len(y_train_sm))\n",
        "print(test_counts_label/ len(y_val_sm))"
      ],
      "metadata": {
        "trusted": true,
        "id": "PYJ0XV7By6sR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LOGISTIC REGRESSION"
      ],
      "metadata": {
        "id": "8UcZ705Sy6sR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List to append the score and then find the average\n",
        "\n",
        "accuracy_lst_reg = []\n",
        "precision_lst_reg = []\n",
        "recall_lst_reg = []\n",
        "f1_lst_reg = []\n",
        "auc_lst_reg = []\n",
        "\n",
        "log_reg_sm = LogisticRegression()\n",
        "#log_reg_params = {}\n",
        "log_reg_params = {\"penalty\": ['l2'],\n",
        "                  'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000],\n",
        "                  'class_weight': ['balanced',None],\n",
        "                  'solver':['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
        "\n",
        "rand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n",
        "\n",
        "\n",
        "for train, val in sss.split(X_train_sm, y_train_sm):\n",
        "    pipeline_reg = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE happens during Cross Validation not before..\n",
        "    model_reg = pipeline_reg.fit(X_train_sm[train], y_train_sm[train])\n",
        "    best_est_reg = rand_log_reg.best_estimator_\n",
        "    prediction_reg = best_est_reg.predict(X_train_sm[val])\n",
        "\n",
        "    accuracy_lst_reg.append(pipeline_reg.score(X_train_sm[val], y_train_sm[val]))\n",
        "    precision_lst_reg.append(precision_score(y_train_sm[val], prediction_reg))\n",
        "    recall_lst_reg.append(recall_score(y_train_sm[val], prediction_reg))\n",
        "    f1_lst_reg.append(f1_score(y_train_sm[val], prediction_reg))\n",
        "    auc_lst_reg.append(roc_auc_score(y_train_sm[val], prediction_reg))\n",
        "\n",
        "\n",
        "print('---' * 45)\n",
        "print('')\n",
        "print('Logistic Regression (SMOTE) results:')\n",
        "print('')\n",
        "print(\"accuracy: {}\".format(np.mean(accuracy_lst_reg)))\n",
        "print(\"precision: {}\".format(np.mean(precision_lst_reg)))\n",
        "print(\"recall: {}\".format(np.mean(recall_lst_reg)))\n",
        "print(\"f1: {}\".format(np.mean(f1_lst_reg)))\n",
        "print('')\n",
        "print('---' * 45)"
      ],
      "metadata": {
        "trusted": true,
        "id": "_tKwAQLxy6sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing the classification report\n",
        "\n",
        "label = ['Fin.Stable', 'Fin.Unstable']\n",
        "pred_reg_sm = best_est_reg.predict(X_val_sm)\n",
        "print(classification_report(y_val_sm, pred_reg_sm, target_names=label))"
      ],
      "metadata": {
        "trusted": true,
        "id": "9APcK_2iy6sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Precision-Recall curve\n",
        "\n",
        "y_score_reg = best_est_reg.predict(X_val_sm)\n",
        "\n",
        "average_precision = average_precision_score(y_val_sm, y_score_reg)\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_val_sm, y_score_reg)\n",
        "\n",
        "plt.step(recall, precision, color='r', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall, precision, step='post', alpha=0.2,\n",
        "                 color='#F59B00')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n",
        "          average_precision), fontsize=15)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "j8pk2MTTy6sS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RANDOM FOREST CLASSIFIER"
      ],
      "metadata": {
        "id": "IFphlWjfy6sT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List to append the score and then find the average\n",
        "\n",
        "accuracy_lst_rfc = []\n",
        "precision_lst_rfc = []\n",
        "recall_lst_rfc = []\n",
        "f1_lst_rfc = []\n",
        "auc_lst_rfc = []\n",
        "\n",
        "rfc_sm = RandomForestClassifier()\n",
        "#rfc_params = {}\n",
        "rfc_params = {'max_features' : ['auto', 'sqrt', 'log2'],\n",
        "              'random_state' : [42],\n",
        "              'class_weight' : ['balanced','balanced_subsample'],\n",
        "              'criterion' : ['gini', 'entropy'],\n",
        "              'bootstrap' : [True,False]}\n",
        "\n",
        "\n",
        "rand_rfc = RandomizedSearchCV(rfc_sm, rfc_params, n_iter=4)\n",
        "\n",
        "\n",
        "for train, val in sss.split(X_train_sm, y_train_sm):\n",
        "    pipeline_rfc = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_rfc) # SMOTE happens during Cross Validation not before..\n",
        "    model_rfc = pipeline_rfc.fit(X_train_sm, y_train_sm)\n",
        "    best_est_rfc = rand_rfc.best_estimator_\n",
        "    prediction_rfc = best_est_rfc.predict(X_train_sm[val])\n",
        "\n",
        "    accuracy_lst_rfc.append(pipeline_rfc.score(X_train_sm[val], y_train_sm[val]))\n",
        "    precision_lst_rfc.append(precision_score(y_train_sm[val], prediction_rfc))\n",
        "    recall_lst_rfc.append(recall_score(y_train_sm[val], prediction_rfc))\n",
        "    f1_lst_rfc.append(f1_score(y_train_sm[val], prediction_rfc))\n",
        "    auc_lst_rfc.append(roc_auc_score(y_train_sm[val], prediction_rfc))\n",
        "\n",
        "print('---' * 45)\n",
        "print('')\n",
        "print(\"accuracy: {}\".format(np.mean(accuracy_lst_rfc)))\n",
        "print(\"precision: {}\".format(np.mean(precision_lst_rfc)))\n",
        "print(\"recall: {}\".format(np.mean(recall_lst_rfc)))\n",
        "print(\"f1: {}\".format(np.mean(f1_lst_rfc)))\n",
        "print('---' * 45)"
      ],
      "metadata": {
        "trusted": true,
        "id": "6MZX0RZpy6sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote_prediction_rfc = best_est_rfc.predict(X_val_sm)\n",
        "print(classification_report(y_val_sm, smote_prediction_rfc, target_names=label))"
      ],
      "metadata": {
        "trusted": true,
        "id": "q3MqC8e2y6sT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Precision-Recall curve\n",
        "\n",
        "y_score_rfc = best_est_rfc.predict(X_val_sm)\n",
        "\n",
        "average_precision_rfc = average_precision_score(y_val_sm, y_score_rfc)\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "\n",
        "precision_rfc, recall_rfc, _ = precision_recall_curve(y_val_sm, y_score_rfc)\n",
        "\n",
        "plt.step(recall_rfc, precision_rfc, color='r', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall_rfc, precision_rfc, step='post', alpha=0.2,\n",
        "                 color='#F59B00')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n",
        "          average_precision_rfc), fontsize=15)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "6HmlOR06y6sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## XGBOOST"
      ],
      "metadata": {
        "id": "lzvH1Samy6sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List to append the score and then find the average\n",
        "\n",
        "accuracy_lst_xgb = []\n",
        "precision_lst_xgb = []\n",
        "recall_lst_xgb = []\n",
        "f1_lst_xgb = []\n",
        "auc_lst_xgb = []\n",
        "\n",
        "xgb_sm = xgb.XGBClassifier(random_state = 42)\n",
        "xgb_params = {'eta' : [0.1,0.01,0.001],  # Learning rate\n",
        "              'eval_metric': ['logloss'],\n",
        "              'max_depth' : [3,6,9],\n",
        "              'lambda' : [1,1.5,2],      # L2 regularization (higher values make model more conservative)\n",
        "              'alpha' : [0,0.5,1]}        # L1 regularization (higher values make model more conservative)\n",
        "              #'reg' : ['squarederror']}\n",
        "              #'random_state': [42]}\n",
        "\n",
        "rand_xgb = RandomizedSearchCV(xgb_sm, xgb_params, n_iter=4)\n",
        "\n",
        "\n",
        "for train, val in sss.split(X_train_sm, y_train_sm):\n",
        "    pipeline_xgb = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_xgb) # SMOTE happens during Cross Validation not before..\n",
        "    model_xgb = pipeline_xgb.fit(X_train_sm, y_train_sm)\n",
        "    best_est_xgb = rand_xgb.best_estimator_\n",
        "    prediction_xgb = best_est_xgb.predict(X_train_sm[val])\n",
        "\n",
        "    accuracy_lst_xgb.append(pipeline_xgb.score(X_train_sm[val], y_train_sm[val]))\n",
        "    precision_lst_xgb.append(precision_score(y_train_sm[val], prediction_xgb))\n",
        "    recall_lst_xgb.append(recall_score(y_train_sm[val], prediction_xgb))\n",
        "    f1_lst_xgb.append(f1_score(y_train_sm[val], prediction_xgb))\n",
        "    auc_lst_xgb.append(roc_auc_score(y_train_sm[val], prediction_xgb))\n",
        "\n",
        "print('---' * 45)\n",
        "print('')\n",
        "print(\"accuracy: {}\".format(np.mean(accuracy_lst_xgb)))\n",
        "print(\"precision: {}\".format(np.mean(precision_lst_xgb)))\n",
        "print(\"recall: {}\".format(np.mean(recall_lst_xgb)))\n",
        "print(\"f1: {}\".format(np.mean(f1_lst_xgb)))\n",
        "print('---' * 45)"
      ],
      "metadata": {
        "trusted": true,
        "id": "RIpVuLady6sU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Printing classification report\n",
        "\n",
        "smote_prediction_xgb = best_est_xgb.predict(X_val_sm)\n",
        "print(classification_report(y_val_sm, smote_prediction_xgb, target_names=label))"
      ],
      "metadata": {
        "trusted": true,
        "id": "VCiSE7W4y6sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Precision-Recall curve\n",
        "\n",
        "y_score_xgb = best_est_xgb.predict(X_val_sm)\n",
        "\n",
        "average_precision_xgb = average_precision_score(y_val_sm, y_score_xgb)\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "\n",
        "precision_xgb, recall_xgb, _ = precision_recall_curve(y_val_sm, y_score_xgb)\n",
        "\n",
        "plt.step(recall_xgb, precision_xgb, color='r', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall_xgb, precision_xgb, step='post', alpha=0.2,\n",
        "                 color='#F59B00')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n",
        "          average_precision_xgb), fontsize=15)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "tbR1IGbDy6sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CATBOOST"
      ],
      "metadata": {
        "id": "bEweD6Dey6sV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List to append the score and then find the average\n",
        "\n",
        "accuracy_lst_cat = []\n",
        "precision_lst_cat = []\n",
        "recall_lst_cat = []\n",
        "f1_lst_cat = []\n",
        "auc_lst_cat = []\n",
        "\n",
        "cat_sm = CatBoostClassifier(verbose = 0)\n",
        "\n",
        "cat_params = {'eval_metric': ['F1'],\n",
        "              'iterations': [100,500,1000],\n",
        "              'learning_rate' : [0.1,0.01,0.001],\n",
        "              'random_seed' : [42],\n",
        "              'auto_class_weights' : ['Balanced','SqrtBalanced']\n",
        "             }\n",
        "\n",
        "\n",
        "rand_cat = RandomizedSearchCV(cat_sm, cat_params, n_iter=4)\n",
        "\n",
        "\n",
        "for train, val in sss.split(X_train_sm, y_train_sm):\n",
        "    pipeline_cat = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_cat) # SMOTE happens during Cross Validation not before..\n",
        "    model_cat = pipeline_cat.fit(X_train_sm, y_train_sm)\n",
        "    best_est_cat = rand_cat.best_estimator_\n",
        "    prediction_cat = best_est_cat.predict(X_train_sm[val])\n",
        "\n",
        "    accuracy_lst_cat.append(pipeline_cat.score(X_train_sm[val], y_train_sm[val]))\n",
        "    precision_lst_cat.append(precision_score(y_train_sm[val], prediction_cat))\n",
        "    recall_lst_cat.append(recall_score(y_train_sm[val], prediction_cat))\n",
        "    f1_lst_cat.append(f1_score(y_train_sm[val], prediction_cat))\n",
        "    auc_lst_cat.append(roc_auc_score(y_train_sm[val], prediction_cat))\n",
        "\n",
        "print('---' * 45)\n",
        "print('')\n",
        "print(\"accuracy: {}\".format(np.mean(accuracy_lst_cat)))\n",
        "print(\"precision: {}\".format(np.mean(precision_lst_cat)))\n",
        "print(\"recall: {}\".format(np.mean(recall_lst_cat)))\n",
        "print(\"f1: {}\".format(np.mean(f1_lst_cat)))\n",
        "print('---' * 45)"
      ],
      "metadata": {
        "trusted": true,
        "id": "GJ9j4TwSy6sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "smote_prediction_cat = best_est_cat.predict(X_val_sm)\n",
        "print(classification_report(y_val_sm, smote_prediction_cat, target_names=label))"
      ],
      "metadata": {
        "trusted": true,
        "id": "oF1U_I20y6sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting Precision-Recall curve\n",
        "\n",
        "smote_prediction_cat = best_est_cat.predict(X_val_sm)\n",
        "\n",
        "average_precision_cat = average_precision_score(y_val_sm, smote_prediction_cat)\n",
        "\n",
        "fig = plt.figure(figsize=(12,6))\n",
        "\n",
        "precision_cat, recall_cat, _ = precision_recall_curve(y_val_sm, smote_prediction_cat)\n",
        "\n",
        "plt.step(recall_cat, precision_cat, color='r', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall_cat, precision_cat, step='post', alpha=0.2,\n",
        "                 color='#F59B00')\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.title('OverSampling Precision-Recall curve: \\n Average Precision-Recall Score ={0:0.2f}'.format(\n",
        "          average_precision_cat), fontsize=15)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "TEeKi0n0y6sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## RESULTS"
      ],
      "metadata": {
        "id": "pm1hvP5Uy6sW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve\n",
        "\n",
        "log_fpr, log_tpr, log_thresold = roc_curve(y_val_sm, pred_reg_sm)\n",
        "rfc_fpr, rfc_tpr, rfc_threshold = roc_curve(y_val_sm, smote_prediction_rfc)\n",
        "xgb_fpr, xgb_tpr, xgb_thresold = roc_curve(y_val_sm, smote_prediction_xgb)\n",
        "cat_fpr, cat_tpr, cat_thresold = roc_curve(y_val_sm, smote_prediction_cat)\n",
        "\n",
        "\n",
        "def graph_roc_curve_multiple(log_fpr, log_tpr, rfc_fpr, rfc_tpr,xgb_fpr, xgb_tpr,cat_fpr, cat_tpr):\n",
        "    plt.figure(figsize=(20,8))\n",
        "    plt.title('ROC Curve', fontsize=14)\n",
        "    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, pred_reg_sm)))\n",
        "    plt.plot(xgb_fpr, xgb_tpr, label='XGBoost Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, smote_prediction_xgb)))\n",
        "    plt.plot(cat_fpr, cat_tpr, label='CatBoost Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, smote_prediction_cat)))\n",
        "    plt.plot(rfc_fpr, rfc_tpr, label='Random Forest Classifier Score: {:.4f}'.format(roc_auc_score(y_val_sm, smote_prediction_rfc)))\n",
        "    plt.plot([0, 1], [0, 1], 'k--')\n",
        "    plt.axis([-0.01, 1, 0, 1])\n",
        "    plt.xlabel('False Positive Rate', fontsize=13)\n",
        "    plt.ylabel('True Positive Rate', fontsize=13)\n",
        "    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n",
        "                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n",
        "                )\n",
        "    plt.legend()\n",
        "\n",
        "graph_roc_curve_multiple(log_fpr, log_tpr, rfc_fpr, rfc_tpr,xgb_fpr, xgb_tpr,cat_fpr, cat_tpr)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "GLCWoUHuy6sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting confusion matrix for each classifier\n",
        "\n",
        "conf_mx0 = confusion_matrix(y_val_sm,pred_reg_sm)\n",
        "conf_mx1 = confusion_matrix(y_val_sm,smote_prediction_rfc)\n",
        "conf_mx2 = confusion_matrix(y_val_sm,smote_prediction_xgb)\n",
        "conf_mx3 = confusion_matrix(y_val_sm,smote_prediction_cat)\n",
        "\n",
        "heat_cm0 = pd.DataFrame(conf_mx0, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\n",
        "heat_cm0.index.name = 'Actual'\n",
        "heat_cm0.columns.name = 'Predicted'\n",
        "\n",
        "heat_cm1 = pd.DataFrame(conf_mx1, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\n",
        "heat_cm1.index.name = 'Actual'\n",
        "heat_cm1.columns.name = 'Predicted'\n",
        "\n",
        "heat_cm2 = pd.DataFrame(conf_mx2, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\n",
        "heat_cm2.index.name = 'Actual'\n",
        "heat_cm2.columns.name = 'Predicted'\n",
        "\n",
        "heat_cm3 = pd.DataFrame(conf_mx3, columns=np.unique(y_val_sm), index = np.unique(y_val_sm))\n",
        "heat_cm3.index.name = 'Actual'\n",
        "heat_cm3.columns.name = 'Predicted'\n",
        "\n",
        "f, ax = plt.subplots(1, 4, figsize=(20,8))\n",
        "f.subplots_adjust(left=None, bottom=None, right= 2, top=None, wspace=None, hspace= None)\n",
        "\n",
        "sns.heatmap(heat_cm0, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[0])\n",
        "ax[0].set_title('Logistic Regression', fontsize = 20)\n",
        "sns.heatmap(heat_cm1, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[1])\n",
        "ax[1].set_title('Random Forest Classifier', fontsize = 20)\n",
        "sns.heatmap(heat_cm2, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[2])\n",
        "ax[2].set_title('XGBoost Classifier', fontsize = 20)\n",
        "sns.heatmap(heat_cm3, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[3])\n",
        "ax[3].set_title('Catboot Classifier', fontsize = 20)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "joUsesYdy6sX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê²€ì¦ ì„¸íŠ¸ì˜ ê²°ê³¼ì—ì„œ ë³¼ ìˆ˜ ìˆë“¯ì´, ëª¨ë“  ëª¨ë¸ì€ íŒŒì‚°ì— ê°€ê¹Œìš´ íšŒì‚¬ë¥¼ íƒì§€í•˜ëŠ” ë° ì—¬ì „íˆ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ì†Œìˆ˜ ì§‘ë‹¨ì— ëŒ€í•œ ë” ë§ì€ ê´€ì°°ì„ ì¸ì‹í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì€ ë¡œì§€ìŠ¤í‹± íšŒê·€ì´ì§€ë§Œ, ì •ë°€ë„ ì¸¡ë©´ì—ì„œ í° ë¹„ìš©ì´ ë“­ë‹ˆë‹¤(í—ˆìœ„ ìŒìˆ˜ê°€ ë§ì´ ì¡´ì¬í•¨). ì˜¤ë¥˜ê°€ ì¡´ì¬í•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  ì´ ê²½ìš° íŒŒì‚°ì— ê°€ê¹Œìš´ ê´€ì°°ì´ ì•„ë‹Œ ê²ƒì„ ê·¸ ë°˜ëŒ€ì˜ ê²½ìš°ë³´ë‹¤ íŒŒì‚°ì— ê°€ê¹Œìš´ ê²ƒìœ¼ë¡œ ì‹ë³„í•˜ëŠ” ê²ƒì´ ë” ë‚«ë‹¤ê³  ìƒê°í•˜ë¯€ë¡œ ìœ ìš©í•œ ëª¨ë¸ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
      ],
      "metadata": {
        "id": "FusHReGCy6sY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "\n",
        "test_pred_lr = best_est_reg.predict(X_test)\n",
        "#test_pred_rf = best_est_rfc.predict(X_test)\n",
        "#test_pred_xgb = best_est_xgb.predict(X_test)\n",
        "test_pred_cat = best_est_cat.predict(X_test)"
      ],
      "metadata": {
        "trusted": true,
        "id": "j4mr4EIxy6sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting confusion matrix for each classifier\n",
        "\n",
        "conf_mx0 = confusion_matrix(y_test,test_pred_lr)\n",
        "conf_mx1 = confusion_matrix(y_test,test_pred_cat)\n",
        "\n",
        "heat_cm0 = pd.DataFrame(conf_mx0, columns=np.unique(y_test), index = np.unique(y_test))\n",
        "heat_cm0.index.name = 'Actual'\n",
        "heat_cm0.columns.name = 'Predicted'\n",
        "\n",
        "heat_cm1 = pd.DataFrame(conf_mx1, columns=np.unique(y_test), index = np.unique(y_test))\n",
        "heat_cm1.index.name = 'Actual'\n",
        "heat_cm1.columns.name = 'Predicted'\n",
        "\n",
        "f, ax = plt.subplots(1, 2, figsize=(15,8))\n",
        "f.subplots_adjust(left=None, bottom=None, right= 2, top=None, wspace=None, hspace= None)\n",
        "\n",
        "sns.heatmap(heat_cm0, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[0])\n",
        "ax[0].set_title('Logistic Regression', fontsize = 20)\n",
        "sns.heatmap(heat_cm1, cmap=\"Blues\", annot=True, annot_kws={\"size\": 16},fmt='g', ax = ax[1])\n",
        "ax[1].set_title('Catboot Classifier', fontsize = 20)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "sUqg_xYIy6sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, test_pred_lr, target_names=label))"
      ],
      "metadata": {
        "trusted": true,
        "id": "ba182xO0y6sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, test_pred_cat, target_names=label))"
      ],
      "metadata": {
        "trusted": true,
        "id": "6nCNaDbhy6sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ê²€ì¦ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ í‘œì‹œëœ ê²ƒê³¼ ì •í™•íˆ ì¼ì¹˜í•˜ê²Œ ìº£ë¶€ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ë©´ ê³ ë ¤ëœ ì§€í‘œ(F1)ê°€ ì–´ë–»ê²Œ ë” ë†’ì€ì§€ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³  ì´ ê²½ìš° ì†Œìˆ˜ì ê³„ì¸µì´ íŒŒì‚°ì— ê·¼ì ‘í•˜ì§€ ì•Šì€ ì¼ë¶€ ê¸°ì—…ì„ íŒŒì‚°ì— ê·¼ì ‘í•œ ê²ƒìœ¼ë¡œ ì˜ëª» ë¶„ë¥˜í•˜ëŠ” ê²ƒì„ ë” ì˜ ì¸ì‹í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ë¡œì§€ìŠ¤í‹± íšŒê·€ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ìµœì„ ì˜ ê²°ì •ì…ë‹ˆë‹¤.\n",
        "\n",
        "ì§€ì›í•´ ì£¼ì…”ì„œ ê°ì‚¬ë“œë¦¬ë©° ë‹¤ìŒ ë…¸íŠ¸ë¶ìœ¼ë¡œ ëµ™ê² ìŠµë‹ˆë‹¤!"
      ],
      "metadata": {
        "id": "GNK-HCxJy6sa"
      }
    }
  ]
}